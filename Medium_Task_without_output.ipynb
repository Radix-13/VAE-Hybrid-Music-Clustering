{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install umap-learn\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (silhouette_score, calinski_harabasz_score,\n",
        "                             davies_bouldin_score, adjusted_rand_score)\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, Model\n",
        "import umap\n",
        "from sklearn.manifold import TSNE\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ],
      "metadata": {
        "id": "HGaFZ9Awnqiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ca3nIrJTltGP"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "# CELL 3: Prepare Spectrogram Data for CNN\n",
        "# This cell handles reshaping and normalizing spectrogram data for use in a Convolutional Neural Network.\n",
        "\n",
        "# Reshape spectrogram for convolutional layers.\n",
        "# The expected input format for a CNN typically includes a channel dimension.\n",
        "# Assuming spectrogram_data is either 2D (n_samples, feature_length) or 3D (n_samples, time_steps, freq_bins).\n",
        "\n",
        "print(\"\\nOriginal spectrogram shape:\", spectrogram_data.shape)\n",
        "\n",
        "# Initialize reshaped data with the original spectrogram in case no reshaping is needed.\n",
        "spec_data_reshaped = spectrogram_data\n",
        "\n",
        "# Check if the spectrogram data is 2D and needs reshaping to 3D + channel.\n",
        "if len(spectrogram_data.shape) == 2:\n",
        "    n_samples, feature_length = spectrogram_data.shape\n",
        "\n",
        "    # Attempt to infer time_steps and freq_bins for reshaping.\n",
        "    # This assumes the feature_length is a product of time_steps and freq_bins.\n",
        "    # A common scenario is feature_length = 128 * 128 or 128 * some_other_dim.\n",
        "\n",
        "    # Try a square-ish reshape if possible.\n",
        "    sqrt_dim = int(math.sqrt(feature_length))\n",
        "\n",
        "    if sqrt_dim * sqrt_dim == feature_length:\n",
        "        # If feature_length is a perfect square, assume square dimensions.\n",
        "        time_steps = freq_bins = sqrt_dim\n",
        "    else:\n",
        "        # Otherwise, default to common mel-spectrogram dimensions if a standard frequency bin count (e.g., 128)\n",
        "        # can divide the feature_length evenly.\n",
        "        default_freq_bins = 128 # Common for Mel-spectrograms\n",
        "        if feature_length % default_freq_bins == 0:\n",
        "            freq_bins = default_freq_bins\n",
        "            time_steps = feature_length // freq_bins\n",
        "        else:\n",
        "            # Fallback if inference is difficult; using the original logic's default if not perfectly divisible.\n",
        "            freq_bins = 128\n",
        "            time_steps = feature_length // freq_bins\n",
        "\n",
        "    # Reshape to (n_samples, time_steps, freq_bins, 1) for CNN input.\n",
        "    spec_data_reshaped = spectrogram_data.reshape(n_samples, time_steps, freq_bins, 1)\n",
        "elif len(spectrogram_data.shape) == 3:\n",
        "    # If already 3D (n_samples, time_steps, freq_bins), add a channel dimension.\n",
        "    spec_data_reshaped = np.expand_dims(spectrogram_data, axis=-1)\n",
        "\n",
        "# Ensure the final shape has 4 dimensions (batch, height, width, channels).\n",
        "if len(spec_data_reshaped.shape) != 4:\n",
        "    print(f\"Warning: Spectrogram data shape after processing is {spec_data_reshaped.shape}, expected 4D for CNN input.\")\n",
        "\n",
        "print(f\"Reshaped spectrogram shape for CNN: {spec_data_reshaped.shape}\")\n",
        "\n",
        "# Normalize spectrogram data to the [0, 1] range.\n",
        "# This is crucial for models using sigmoid activation in the output layer or binary crossentropy loss.\n",
        "# It also helps with model stability and training performance.\n",
        "# Assumes spectrogram values represent non-negative intensities.\n",
        "min_intensity = np.min(spec_data_reshaped)\n",
        "max_intensity = np.max(spec_data_reshaped)\n",
        "\n",
        "# Add a small epsilon to the denominator to prevent division by zero if all values are the same.\n",
        "spec_data_normalized = (spec_data_reshaped - min_intensity) / (max_intensity - min_intensity + 1e-8)\n",
        "\n",
        "print(f\"Spectrogram data normalized. Min: {np.min(spec_data_normalized):.4f}, Max: {np.max(spec_data_normalized):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# CELL 2: Load and Explore Data\n",
        "\n",
        "# Load your files\n",
        "dataset = pd.read_csv('/content/drive/MyDrive/VAE for Hybrid Language Music Clustering/Dataset/dataset.csv')\n",
        "preprocessed_data = np.load('/content/drive/MyDrive/VAE for Hybrid Language Music Clustering/Dataset/processed_ids.npy')\n",
        "spectrogram_data = np.load('/content/drive/MyDrive/VAE for Hybrid Language Music Clustering/Dataset/processed_spectrograms.npy')\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"DATA OVERVIEW\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Dataset shape: {dataset.shape}\")\n",
        "print(f\"Preprocessed data shape: {preprocessed_data.shape}\")\n",
        "print(f\"Spectrogram shape: {spectrogram_data.shape}\")\n",
        "print(f\"\\nDataset columns: {dataset.columns.tolist()}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "print(dataset.head())\n",
        "# Check if there are labels for ARI calculation\n",
        "has_labels = 'label' in dataset.columns or 'genre' in dataset.columns or 'language' in dataset.columns\n",
        "if has_labels:\n",
        "    label_col = 'label' if 'label' in dataset.columns else ('genre' if 'genre' in dataset.columns else 'language')\n",
        "    true_labels = dataset[label_col].values\n",
        "    print(f\"\\nFound labels in column: {label_col}\")\n",
        "    print(f\"Unique labels: {np.unique(true_labels)}\")\n",
        "else:\n",
        "    true_labels = None\n",
        "    print(\"\\nNo labels found. ARI will not be computed.\")"
      ],
      "metadata": {
        "id": "uQHGBzynluV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# CELL 9: Build Hybrid VAE (Audio + Lyrics)\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"BUILDING HYBRID VAE (AUDIO + LYRICS)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# --- 1. Prepare Lyrics Input Branch ---\n",
        "# This branch processes the normalized lyrics embeddings.\n",
        "lyrics_dim = lyrics_embeddings_normalized.shape[1] # Determine input dimension from preprocessed lyrics\n",
        "lyrics_input = keras.Input(shape=(lyrics_dim,), name='lyrics_input')\n",
        "\n",
        "# Lyrics encoder sub-network (Dense layers)\n",
        "# Extracts features from lyrics embeddings into a lower-dimensional space.\n",
        "lyrics_x = layers.Dense(256, activation='relu', name='lyrics_dense1')(lyrics_input)\n",
        "lyrics_x = layers.Dropout(0.3, name='lyrics_dropout1')(lyrics_x) # Dropout for regularization\n",
        "lyrics_x = layers.Dense(128, activation='relu', name='lyrics_dense2')(lyrics_x)\n",
        "lyrics_encoded = layers.Dense(32, activation='relu', name='lyrics_encoded')(lyrics_x)\n",
        "\n",
        "# --- 2. Integrate Spectrogram Encoder ---\n",
        "# The pre-trained convolutional encoder (conv_encoder) is used to process spectrograms.\n",
        "# This reuses the learned audio representations.\n",
        "spec_input = keras.Input(shape=input_shape, name='spec_input')\n",
        "\n",
        "# Pass spectrogram input through the existing conv_encoder.\n",
        "# We only need the sampled latent vector (z_s) for concatenation here.\n",
        "z_mean_s, z_log_var_s, z_s = conv_encoder(spec_input)\n",
        "\n",
        "# --- 3. Fuse Modalities ---\n",
        "# Concatenate the latent representations from both audio (spectrogram) and lyrics.\n",
        "combined = layers.Concatenate(name='combined_features')([z_s, lyrics_encoded])\n",
        "\n",
        "# Fusion layer to process the combined features.\n",
        "fusion = layers.Dense(128, activation='relu', name='fusion_dense1')(combined)\n",
        "fusion = layers.Dropout(0.3, name='fusion_dropout1')(fusion)\n",
        "\n",
        "# --- 4. Hybrid Latent Space ---\n",
        "# Define the hybrid latent space, similar to a standard VAE.\n",
        "# This latent space combines information from both modalities.\n",
        "hybrid_z_mean = layers.Dense(latent_dim, name='hybrid_z_mean')(fusion)\n",
        "hybrid_z_log_var = layers.Dense(latent_dim, name='hybrid_z_log_var')(fusion)\n",
        "\n",
        "# Sample from the hybrid latent distribution using the reparameterization trick.\n",
        "hybrid_z = Sampling(name='hybrid_z_sampling')([hybrid_z_mean, hybrid_z_log_var])\n",
        "\n",
        "# --- 5. Hybrid Encoder Model ---\n",
        "# The hybrid encoder takes both spectrogram and lyrics as input\n",
        "# and outputs the parameters of the hybrid latent space.\n",
        "hybrid_encoder = Model(inputs=[spec_input, lyrics_input],\n",
        "                       outputs=[hybrid_z_mean, hybrid_z_log_var, hybrid_z],\n",
        "                       name='hybrid_encoder')\n",
        "hybrid_encoder.summary()\n",
        "\n",
        "# --- 6. Hybrid Decoder Model ---\n",
        "# The hybrid decoder takes a sample from the hybrid latent space\n",
        "# and reconstructs both the spectrogram and the lyrics embeddings.\n",
        "hybrid_latent_input = keras.Input(shape=(latent_dim,), name='hybrid_latent_input')\n",
        "\n",
        "# Reconstruct spectrogram using the pre-trained convolutional decoder.\n",
        "spec_decoded = conv_decoder(hybrid_latent_input)\n",
        "\n",
        "# Decode back to lyrics embeddings using a separate sub-network.\n",
        "lyrics_decoder_x = layers.Dense(128, activation='relu', name='lyrics_decode_dense1')(hybrid_latent_input)\n",
        "lyrics_decoder_x = layers.Dense(256, activation='relu', name='lyrics_decode_dense2')(lyrics_decoder_x)\n",
        "lyrics_decoded = layers.Dense(lyrics_dim, activation='sigmoid', name='lyrics_decoded')(lyrics_decoder_x)\n",
        "\n",
        "hybrid_decoder = Model(inputs=hybrid_latent_input, outputs=[spec_decoded, lyrics_decoded], name='hybrid_decoder')\n",
        "hybrid_decoder.summary()\n",
        "\n",
        "# --- 7. Custom Hybrid VAE Class ---\n",
        "# This custom Keras Model encapsulates the hybrid encoder and decoder,\n",
        "# and defines the VAE's loss function for both modalities plus KL divergence.\n",
        "class HybridVAE(keras.Model):\n",
        "    \"\"\"Hybrid Variational Autoencoder (VAE) model for audio and text modalities.\"\"\"\n",
        "\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        # Initialize metrics to track losses for each modality and the total loss.\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_spec_loss_tracker = keras.metrics.Mean(name=\"reconstruction_spec_loss\")\n",
        "        self.reconstruction_lyrics_loss_tracker = keras.metrics.Mean(name=\"reconstruction_lyrics_loss\")\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Forward pass through the Hybrid VAE. (Note: Loss calculated in train_step)\"\"\"\n",
        "        # Inputs are expected to be a list: [spectrogram_data, lyrics_data]\n",
        "        spectrogram_data, lyrics_data = inputs\n",
        "\n",
        "        # Encode both modalities to get latent space parameters and sample.\n",
        "        z_mean, z_log_var, z = self.encoder([spectrogram_data, lyrics_data])\n",
        "        # Decode the sampled latent vector to reconstruct both modalities.\n",
        "        spec_reconstruction, lyrics_reconstruction = self.decoder(z)\n",
        "        return spec_reconstruction, lyrics_reconstruction\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        \"\"\"Returns the list of metrics monitored during training/testing.\"\"\"\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_spec_loss_tracker,\n",
        "            self.reconstruction_lyrics_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        \"\"\"Performs a single training step for the Hybrid VAE.\"\"\"\n",
        "        # 'data' is expected to be a tuple: ([spectrogram_inputs, lyrics_inputs], [spectrogram_targets, lyrics_targets])\n",
        "        # where inputs and targets are typically the same for VAEs.\n",
        "        input_data, target_data = data\n",
        "        spec_data_batch, lyrics_data_batch = input_data\n",
        "        spec_target_batch, lyrics_target_batch = target_data\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Encode the combined input data.\n",
        "            z_mean, z_log_var, z = self.encoder([spec_data_batch, lyrics_data_batch])\n",
        "            # Decode the sampled latent vector to get reconstructions of both modalities.\n",
        "            spec_reconstruction, lyrics_reconstruction = self.decoder(z)\n",
        "\n",
        "            # Calculate Reconstruction Loss for Spectrograms.\n",
        "            # Uses binary_crossentropy as spectrograms were normalized to [0,1].\n",
        "            spec_recon_loss = keras.ops.mean(\n",
        "                keras.ops.sum(\n",
        "                    keras.losses.binary_crossentropy(spec_target_batch, spec_reconstruction),\n",
        "                    axis=(1, 2) # Sum across spatial dimensions (height, width) for each sample\n",
        "                ) # No reduction at batch level yet\n",
        "            )\n",
        "            # Calculate Reconstruction Loss for Lyrics Embeddings.\n",
        "            # Uses Mean Squared Error (MSE) as lyrics embeddings are continuous values.\n",
        "            lyrics_recon_loss = keras.ops.mean(\n",
        "                keras.losses.mse(lyrics_target_batch, lyrics_reconstruction)\n",
        "            )\n",
        "\n",
        "            # Calculate KL Divergence Loss.\n",
        "            # Encourages the hybrid latent distribution to be close to a standard normal distribution.\n",
        "            kl_loss = -0.5 * keras.ops.mean(\n",
        "                keras.ops.sum(\n",
        "                    1 + z_log_var - keras.ops.square(z_mean) - keras.ops.exp(z_log_var),\n",
        "                    axis=1 # Sum across latent dimensions\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Total Hybrid VAE loss is the sum of reconstruction losses and KL divergence loss.\n",
        "            total_loss = spec_recon_loss + lyrics_recon_loss + kl_loss\n",
        "\n",
        "        # Compute gradients and apply them to update model weights.\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "\n",
        "        # Update the state of the loss metrics.\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_spec_loss_tracker.update_state(spec_recon_loss)\n",
        "        self.reconstruction_lyrics_loss_tracker.update_state(lyrics_recon_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "\n",
        "        # Return current metric values.\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_spec_loss\": self.reconstruction_spec_loss_tracker.result(),\n",
        "            \"reconstruction_lyrics_loss\": self.reconstruction_lyrics_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n",
        "\n",
        "    def test_step(self, data):\n",
        "        \"\"\"Performs a single test/validation step for the Hybrid VAE.\"\"\"\n",
        "        # 'data' is expected to be a tuple: ([spectrogram_inputs, lyrics_inputs], [spectrogram_targets, lyrics_targets])\n",
        "        input_data, target_data = data\n",
        "        spec_data_batch, lyrics_data_batch = input_data\n",
        "        spec_target_batch, lyrics_target_batch = target_data\n",
        "\n",
        "        # Encode the combined input data.\n",
        "        z_mean, z_log_var, z = self.encoder([spec_data_batch, lyrics_data_batch])\n",
        "        # Decode the sampled latent vector to get reconstructions of both modalities.\n",
        "        spec_reconstruction, lyrics_reconstruction = self.decoder(z)\n",
        "\n",
        "        # Calculate Reconstruction Loss for Spectrograms.\n",
        "        spec_recon_loss = keras.ops.mean(\n",
        "            keras.ops.sum(\n",
        "                keras.losses.binary_crossentropy(spec_target_batch, spec_reconstruction),\n",
        "                axis=(1, 2)\n",
        "            )\n",
        "        )\n",
        "        # Calculate Reconstruction Loss for Lyrics Embeddings.\n",
        "        lyrics_recon_loss = keras.ops.mean(\n",
        "            keras.losses.mse(lyrics_target_batch, lyrics_reconstruction)\n",
        "        )\n",
        "\n",
        "        # Calculate KL Divergence Loss.\n",
        "        kl_loss = -0.5 * keras.ops.mean(\n",
        "            keras.ops.sum(\n",
        "                1 + z_log_var - keras.ops.square(z_mean) - keras.ops.exp(z_log_var),\n",
        "                axis=1\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Total Hybrid VAE loss.\n",
        "        total_loss = spec_recon_loss + lyrics_recon_loss + kl_loss\n",
        "\n",
        "        # Update the state of the loss metrics.\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_spec_loss_tracker.update_state(spec_recon_loss)\n",
        "        self.reconstruction_lyrics_loss_tracker.update_state(lyrics_recon_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "\n",
        "        # Return current metric values.\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_spec_loss\": self.reconstruction_spec_loss_tracker.result(),\n",
        "            \"reconstruction_lyrics_loss\": self.reconstruction_lyrics_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n"
      ],
      "metadata": {
        "id": "lwfLTXO1oK-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwroAPNGmj9E"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ========================================\n",
        "# CELL 4: Prepare Lyrics Embeddings (if available)\n",
        "# ========================================\n",
        "\n",
        "# Define the base path for data files. This path is crucial for locating various assets.\n",
        "# This variable should be set in a preceding cell or globally for the notebook.\n",
        "data_path = '/content/drive/MyDrive/VAE for Hybrid Language Music Clustering'\n",
        "\n",
        "# Construct the full file path for the lyrics embeddings.\n",
        "# Corrected file name based on directory listing.\n",
        "lyrics_embedding_file_path = os.path.join(data_path, '/content/drive/MyDrive/VAE for Hybrid Language Music Clustering/Dataset/Copy of lyrics_embeddings.npy')\n",
        "\n",
        "# Initialize flags and variables.\n",
        "# `use_lyrics_embeddings_feature` indicates if a lyrics-based feature will be incorporated.\n",
        "# `lyrics_embeddings_normalized` will store the processed lyrics features.\n",
        "use_lyrics_embeddings_feature = False\n",
        "lyrics_embeddings_normalized = None\n",
        "\n",
        "# --- Attempt to Load Existing Lyrics Embeddings ---\n",
        "if os.path.exists(lyrics_embedding_file_path):\n",
        "    print(f\"\\n--- Loading Lyrics Embeddings ---\")\n",
        "    print(f\"Found lyrics embeddings file at: {lyrics_embedding_file_path}\")\n",
        "\n",
        "    # Load the pre-computed embeddings.\n",
        "    lyrics_embeddings = np.load(lyrics_embedding_file_path)\n",
        "    print(f\"Original lyrics embeddings shape: {lyrics_embeddings.shape}\")\n",
        "\n",
        "    # Normalize the embeddings using StandardScaler.\n",
        "    # Normalization (mean=0, std=1) helps ensure features contribute equally to the model\n",
        "    # and can improve convergence during training.\n",
        "    scaler = StandardScaler()\n",
        "    lyrics_embeddings_normalized = scaler.fit_transform(lyrics_embeddings)\n",
        "\n",
        "    use_lyrics_embeddings_feature = True\n",
        "    print(\"Lyrics embeddings loaded and normalized successfully.\")\n",
        "    print(f\"Normalized lyrics embeddings shape: {lyrics_embeddings_normalized.shape}\")\n",
        "\n",
        "# --- Create Dummy Embeddings if File Not Found ---\n",
        "else:\n",
        "    print(f\"\\n--- Generating Dummy Lyrics Embeddings ---\")\n",
        "    print(f\"No lyrics embeddings file found at: {lyrics_embedding_file_path}.\")\n",
        "    print(\"Creating dummy lyrics embeddings as a fallback. This is a placeholder for actual NLP features.\")\n",
        "\n",
        "    # `preprocessed_data` is assumed to be an array of preprocessed IDs (1D).\n",
        "    # If `preprocessed_data` were a 2D array of features, we might try to extract from it.\n",
        "    # However, since it's 1D, we generate a simple placeholder.\n",
        "\n",
        "    # Determine the number of samples from `preprocessed_data`.\n",
        "    num_samples = preprocessed_data.shape[0]\n",
        "\n",
        "    # Create a 2D array of zeros. Each sample will have one dummy feature.\n",
        "    # In a real scenario, this would be replaced by actual NLP-derived features from lyrics text.\n",
        "    dummy_embeddings_input = np.zeros((num_samples, 1))\n",
        "    print(f\"Created a dummy input array of shape: {dummy_embeddings_input.shape} (all zeros).\")\n",
        "\n",
        "    # Normalize these dummy embeddings. Even dummy features should be scaled for consistency.\n",
        "    scaler = StandardScaler()\n",
        "    lyrics_embeddings_normalized = scaler.fit_transform(dummy_embeddings_input)\n",
        "\n",
        "    use_lyrics_embeddings_feature = True # A lyrics feature (even if dummy) is now prepared.\n",
        "    print(f\"Dummy lyrics embeddings created and normalized. Shape: {lyrics_embeddings_normalized.shape}\")\n",
        "\n",
        "# --- Final Status Report ---\n",
        "print(f\"\\n--- Lyrics Embeddings Preparation Complete ---\")\n",
        "print(f\"A lyrics embeddings feature will be used in the model: {use_lyrics_embeddings_feature}\")\n",
        "print(f\"Shape of final lyrics embeddings feature: {lyrics_embeddings_normalized.shape if lyrics_embeddings_normalized is not None else 'Not available'}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a007fe04"
      },
      "source": [
        "import os\n",
        "\n",
        "data_path = '/content/drive/MyDrive/VAE for Hybrid Language Music Clustering'\n",
        "lyrics_embedding_file_path = os.path.join(data_path, '/content/drive/MyDrive/VAE for Hybrid Language Music Clustering/Dataset/Copy of lyrics_embeddings.npy')\n",
        "\n",
        "print(f\"Checking for file: {lyrics_embedding_file_path}\")\n",
        "if os.path.exists(lyrics_embedding_file_path):\n",
        "    print(\"SUCCESS: lyrics_embeddings.npy found at the specified path!\")\n",
        "else:\n",
        "    print(\"FAILURE: lyrics_embeddings.npy NOT found at the specified path.\")\n",
        "    print(\"\\nListing contents of the directory:\")\n",
        "    try:\n",
        "        for item in os.listdir(data_path):\n",
        "            print(f\"- {item}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The directory {data_path} was not found. Please ensure your Google Drive is mounted correctly and the path is valid.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while listing directory contents: {e}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# CELL 5: Build Convolutional VAE for Spectrograms\n",
        "# ========================================\n",
        "\n",
        "# Define the dimensionality of the latent space.\n",
        "# This is a hyperparameter that determines the complexity of the learned representation.\n",
        "latent_dim = 64\n",
        "\n",
        "# --- Sampling Layer ---\n",
        "# This custom layer implements the reparameterization trick,\n",
        "# which allows for backpropagation through the sampling process.\n",
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        # Get batch size and dimensionality of the latent space.\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        # Generate random normal epsilon for reparameterization.\n",
        "        epsilon = tf.random.normal(shape=(batch, dim))\n",
        "        # z = z_mean + exp(0.5 * z_log_var) * epsilon\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "# --- Encoder Model ---\n",
        "# The encoder takes the input spectrogram and maps it to the parameters\n",
        "# (mean and log-variance) of the latent space distribution.\n",
        "\n",
        "# Dynamically determine input shape from preprocessed spectrogram data.\n",
        "# Assumes spec_data_normalized is already available and has shape (batch, height, width, channels).\n",
        "input_shape = spec_data_normalized.shape[1:]  # Exclude batch dimension\n",
        "print(f\"\\nBuilding Conv-VAE with input shape: {input_shape}\")\n",
        "\n",
        "encoder_inputs = keras.Input(shape=input_shape, name='encoder_input')\n",
        "\n",
        "# Apply convolutional layers to extract features.\n",
        "# Each Conv2D layer is followed by BatchNormalization to stabilize training.\n",
        "x = layers.Conv2D(32, 3, activation='relu', strides=2, padding='same', name='enc_conv1')(encoder_inputs)\n",
        "x = layers.BatchNormalization(name='enc_bn1')(x)\n",
        "x = layers.Conv2D(64, 3, activation='relu', strides=2, padding='same', name='enc_conv2')(x)\n",
        "x = layers.BatchNormalization(name='enc_bn2')(x)\n",
        "x = layers.Conv2D(128, 3, activation='relu', strides=2, padding='same', name='enc_conv3')(x)\n",
        "x = layers.BatchNormalization(name='enc_bn3')(x)\n",
        "x = layers.Conv2D(256, 3, activation='relu', strides=2, padding='same', name='enc_conv4')(x)\n",
        "x = layers.BatchNormalization(name='enc_bn4')(x)\n",
        "\n",
        "# Store the shape before flattening to reconstruct it later in the decoder.\n",
        "shape_before_flattening = x.shape[1:]\n",
        "\n",
        "# Flatten the feature maps and apply a dense layer.\n",
        "x = layers.Flatten(name='enc_flatten')(x)\n",
        "x = layers.Dense(512, activation='relu', name='enc_dense')(x)\n",
        "\n",
        "# Project to the latent space's mean and log-variance.\n",
        "z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
        "z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
        "\n",
        "# Sample from the latent distribution using the reparameterization trick.\n",
        "z = Sampling(name='z_sampling')([z_mean, z_log_var]) # Sampled latent vector\n",
        "\n",
        "# Create the encoder model.\n",
        "conv_encoder = Model(encoder_inputs, [z_mean, z_log_var, z], name='conv_encoder')\n",
        "conv_encoder.summary()\n",
        "\n",
        "# --- Decoder Model ---\n",
        "# The decoder takes a sampled latent vector and reconstructs the spectrogram.\n",
        "\n",
        "latent_inputs = keras.Input(shape=(latent_dim,), name='latent_input')\n",
        "\n",
        "# Apply dense layers to expand the latent vector.\n",
        "x = layers.Dense(512, activation='relu', name='dec_dense1')(latent_inputs)\n",
        "# Reshape to the spatial dimensions needed for deconvolutional layers.\n",
        "x = layers.Dense(np.prod(shape_before_flattening), activation='relu', name='dec_dense2')(x)\n",
        "x = layers.Reshape(shape_before_flattening, name='dec_reshape')(x)\n",
        "\n",
        "# Apply deconvolutional (Conv2DTranspose) layers to upsample and reconstruct.\n",
        "# Each Conv2DTranspose is followed by BatchNormalization.\n",
        "x = layers.Conv2DTranspose(256, 3, activation='relu', strides=2, padding='same', name='dec_conv_transpose1')(x)\n",
        "x = layers.BatchNormalization(name='dec_bn1')(x)\n",
        "x = layers.Conv2DTranspose(128, 3, activation='relu', strides=2, padding='same', name='dec_conv_transpose2')(x)\n",
        "x = layers.BatchNormalization(name='dec_bn2')(x)\n",
        "x = layers.Conv2DTranspose(64, 3, activation='relu', strides=2, padding='same', name='dec_conv_transpose3')(x)\n",
        "x = layers.BatchNormalization(name='dec_bn3')(x)\n",
        "x = layers.Conv2DTranspose(32, 3, activation='relu', strides=2, padding='same', name='dec_conv_transpose4')(x)\n",
        "x = layers.BatchNormalization(name='dec_bn4')(x)\n",
        "\n",
        "# Final layer outputs the reconstructed spectrogram with a single channel.\n",
        "# Using 'sigmoid' activation to ensure output is in [0, 1] range, matching normalized input.\n",
        "decoder_outputs = layers.Conv2DTranspose(1, 3, activation='sigmoid', padding='same', name='decoder_output')(x)\n",
        "\n",
        "# Create the decoder model.\n",
        "conv_decoder = Model(latent_inputs, decoder_outputs, name='conv_decoder')\n",
        "conv_decoder.summary()\n",
        "\n",
        "# --- Convolutional VAE Class ---\n",
        "# This custom Keras Model encapsulates the encoder and decoder,\n",
        "# and defines the VAE's loss function (reconstruction loss + KL divergence).\n",
        "class ConvVAE(keras.Model):\n",
        "    \"\"\"Convolutional Variational Autoencoder (VAE) model.\"\"\"\n",
        "\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        # Initialize metrics to track losses during training.\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        \"\"\"Forward pass through the VAE.\"\"\"\n",
        "        # Encode input to get latent space parameters and sample.\n",
        "        z_mean, z_log_var, z = self.encoder(inputs)\n",
        "        # Decode the sampled latent vector to reconstruct the input.\n",
        "        reconstruction = self.decoder(z)\n",
        "        return reconstruction\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        \"\"\"Returns the list of metrics monitored during training/testing.\"\"\"\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        \"\"\"Performs a single training step for the VAE.\"\"\"\n",
        "        if isinstance(data, tuple):\n",
        "            data = data[0] # Handle cases where data is a (features, labels) tuple\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Encode the input data.\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            # Decode the sampled latent vector to get reconstruction.\n",
        "            reconstruction = self.decoder(z)\n",
        "\n",
        "            # Calculate Reconstruction Loss (Binary Crossentropy).\n",
        "            # Measures how well the VAE reconstructs the input.\n",
        "            reconstruction_loss = keras.ops.mean(\n",
        "                keras.ops.sum(\n",
        "                    keras.losses.binary_crossentropy(data, reconstruction),\n",
        "                    axis=(1, 2) # Sum across spatial dimensions\n",
        "                )\n",
        "            )\n",
        "            # Calculate KL Divergence Loss.\n",
        "            # Encourages the latent distribution to be close to a standard normal distribution.\n",
        "            kl_loss = -0.5 * keras.ops.mean(\n",
        "                keras.ops.sum(\n",
        "                    1 + z_log_var - keras.ops.square(z_mean) - keras.ops.exp(z_log_var),\n",
        "                    axis=1 # Sum across latent dimensions\n",
        "                )\n",
        "            )\n",
        "            # Total VAE loss is the sum of reconstruction loss and KL divergence loss.\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "\n",
        "        # Compute gradients and apply them to update model weights.\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "\n",
        "        # Update the state of the loss metrics.\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "\n",
        "        # Return current metric values.\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n",
        "\n",
        "    def test_step(self, data):\n",
        "        \"\"\"Performs a single test/validation step for the VAE.\"\"\"\n",
        "        if isinstance(data, tuple):\n",
        "            data = data[0] # Handle cases where data is a (features, labels) tuple\n",
        "\n",
        "        # Encode and decode the input data.\n",
        "        z_mean, z_log_var, z = self.encoder(data)\n",
        "        reconstruction = self.decoder(z)\n",
        "\n",
        "        # Calculate Reconstruction Loss.\n",
        "        reconstruction_loss = keras.ops.mean(\n",
        "            keras.ops.sum(\n",
        "                keras.losses.binary_crossentropy(data, reconstruction),\n",
        "                axis=(1, 2)\n",
        "            )\n",
        "        )\n",
        "        # Calculate KL Divergence Loss.\n",
        "        kl_loss = -0.5 * keras.ops.mean(\n",
        "            keras.ops.sum(\n",
        "                1 + z_log_var - keras.ops.square(z_mean) - keras.ops.exp(z_log_var),\n",
        "                axis=1\n",
        "            )\n",
        "        )\n",
        "        # Total VAE loss.\n",
        "        total_loss = reconstruction_loss + kl_loss\n",
        "\n",
        "        # Update the state of the loss metrics.\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "\n",
        "        # Return current metric values.\n",
        "        return {\n",
        "            \"loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }\n"
      ],
      "metadata": {
        "id": "DPTi17f-p5GM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35a4aaa3"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "# CELL 3: Prepare Spectrogram Data for CNN\n",
        "# This cell handles reshaping and normalizing spectrogram data for use in a Convolutional Neural Network.\n",
        "\n",
        "# Reshape spectrogram for convolutional layers.\n",
        "# The expected input format for a CNN typically includes a channel dimension.\n",
        "# Assuming spectrogram_data is either 2D (n_samples, feature_length) or 3D (n_samples, time_steps, freq_bins).\n",
        "\n",
        "print(\"\\nOriginal spectrogram shape:\", spectrogram_data.shape)\n",
        "\n",
        "# Initialize reshaped data with the original spectrogram in case no reshaping is needed.\n",
        "spec_data_reshaped = spectrogram_data\n",
        "\n",
        "# Check if the spectrogram data is 2D and needs reshaping to 3D + channel.\n",
        "if len(spectrogram_data.shape) == 2:\n",
        "    n_samples, feature_length = spectrogram_data.shape\n",
        "\n",
        "    # Attempt to infer time_steps and freq_bins for reshaping.\n",
        "    # This assumes the feature_length is a product of time_steps and freq_bins.\n",
        "    # A common scenario is feature_length = 128 * 128 or 128 * some_other_dim.\n",
        "\n",
        "    # Try a square-ish reshape if possible.\n",
        "    sqrt_dim = int(math.sqrt(feature_length))\n",
        "\n",
        "    if sqrt_dim * sqrt_dim == feature_length:\n",
        "        # If feature_length is a perfect square, assume square dimensions.\n",
        "        time_steps = freq_bins = sqrt_dim\n",
        "    else:\n",
        "        # Otherwise, default to common mel-spectrogram dimensions if a standard frequency bin count (e.g., 128)\n",
        "        # can divide the feature_length evenly.\n",
        "        default_freq_bins = 128 # Common for Mel-spectrograms\n",
        "        if feature_length % default_freq_bins == 0:\n",
        "            freq_bins = default_freq_bins\n",
        "            time_steps = feature_length // freq_bins\n",
        "        else:\n",
        "            # Fallback if inference is difficult; using the original logic's default if not perfectly divisible.\n",
        "            freq_bins = 128\n",
        "            time_steps = feature_length // freq_bins\n",
        "\n",
        "    # Reshape to (n_samples, time_steps, freq_bins, 1) for CNN input.\n",
        "    spec_data_reshaped = spectrogram_data.reshape(n_samples, time_steps, freq_bins, 1)\n",
        "elif len(spectrogram_data.shape) == 3:\n",
        "    # If already 3D (n_samples, time_steps, freq_bins), add a channel dimension.\n",
        "    spec_data_reshaped = np.expand_dims(spectrogram_data, axis=-1)\n",
        "\n",
        "# Ensure the final shape has 4 dimensions (batch, height, width, channels).\n",
        "if len(spec_data_reshaped.shape) != 4:\n",
        "    print(f\"Warning: Spectrogram data shape after processing is {spec_data_reshaped.shape}, expected 4D for CNN input.\")\n",
        "\n",
        "print(f\"Reshaped spectrogram shape for CNN: {spec_data_reshaped.shape}\")\n",
        "\n",
        "# Normalize spectrogram data to the [0, 1] range.\n",
        "# This is crucial for models using sigmoid activation in the output layer or binary crossentropy loss.\n",
        "# It also helps with model stability and training performance.\n",
        "# Assumes spectrogram values represent non-negative intensities.\n",
        "min_intensity = np.min(spec_data_reshaped)\n",
        "max_intensity = np.max(spec_data_reshaped)\n",
        "\n",
        "# Add a small epsilon to the denominator to prevent division by zero if all values are the same.\n",
        "spec_data_normalized = (spec_data_reshaped - min_intensity) / (max_intensity - min_intensity + 1e-8)\n",
        "\n",
        "print(f\"Spectrogram data normalized. Min: {np.min(spec_data_normalized):.4f}, Max: {np.max(spec_data_normalized):.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# CELL 6: Conv-VAE Loss and Compilation\n",
        "# ========================================\n",
        "\n",
        "# Instantiate the ConvVAE model\n",
        "# This creates an instance of our custom VAE class, combining the encoder and decoder.\n",
        "conv_vae = ConvVAE(encoder=conv_encoder, decoder=conv_decoder)\n",
        "\n",
        "# Compile the model.\n",
        "# For the ConvVAE, the custom loss functions (reconstruction loss + KL divergence)\n",
        "# are calculated internally within the overridden `train_step` and `test_step` methods.\n",
        "# Therefore, we only need to specify an optimizer here.\n",
        "# Adam optimizer is chosen with a small learning rate for stable training.\n",
        "conv_vae.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4))\n",
        "\n",
        "print(\"\\nConv-VAE Model Summary:\")\n",
        "# To display a summary of the entire VAE model, it needs to be built first.\n",
        "# The `build` method expects the input shape (excluding the batch dimension).\n",
        "# `input_shape` was already determined dynamically in CELL 5 from `spec_data_normalized`.\n",
        "# (None,) + input_shape creates a shape like (None, 128, 256, 1) for the batch input.\n",
        "conv_vae.build(input_shape=(None,) + input_shape)\n",
        "conv_vae.summary()\n"
      ],
      "metadata": {
        "id": "ViLAEH4sqN8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# CELL 7: Train Convolutional VAE\n",
        "# ========================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TRAINING CONVOLUTIONAL VAE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Define base path for saving results, assuming it's the same as data_path\n",
        "# Make sure this path exists or create it if necessary\n",
        "base_path = '/content/drive/MyDrive/VAE for Hybrid Language Music Clustering'\n",
        "\n",
        "# Train the ConvVAE model.\n",
        "# The input to the VAE's `fit` method is typically the same data for both features and labels\n",
        "# as it learns to reconstruct its own input (unsupervised learning).\n",
        "conv_history = conv_vae.fit(\n",
        "    spec_data_normalized,    # Input data (spectrograms)\n",
        "    spec_data_normalized,    # Target data (spectrograms for reconstruction)\n",
        "    epochs=50,               # Number of times to iterate over the entire dataset\n",
        "    batch_size=32,           # Number of samples per gradient update\n",
        "    validation_split=0.2,    # Fraction of the training data to be used as validation data\n",
        "    verbose=1                # Display progress bar during training\n",
        ")\n",
        "\n",
        "# --- Plotting Training History ---\n",
        "# Visualize the training and validation loss over epochs to monitor model performance\n",
        "# and identify potential overfitting or underfitting.\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Plot training loss (total loss calculated by the custom train_step)\n",
        "plt.plot(conv_history.history['loss'], label='Training Loss', linewidth=2)\n",
        "\n",
        "# Plot validation loss\n",
        "plt.plot(conv_history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "\n",
        "# Add labels and title for clarity\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Loss', fontsize=12)\n",
        "plt.title('Convolutional VAE Training History', fontsize=14)\n",
        "\n",
        "# Add a legend to distinguish between training and validation loss\n",
        "plt.legend(fontsize=10)\n",
        "\n",
        "# Add a grid for better readability of data points\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Ensure the results directory exists before saving\n",
        "results_dir = os.path.join(base_path, 'results/visualizations')\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "# Save the plot to a file\n",
        "plt.savefig(os.path.join(results_dir, 'conv_vae_training.png'), dpi=300, bbox_inches='tight')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kkOlhFQuqd2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jG7IF5Qx_31B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af6fd8ef"
      },
      "source": [
        "# ========================================\n",
        "# CELL 8: Extract Latent Features from Conv-VAE\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EXTRACTING LATENT FEATURES FROM CONVOLUTIONAL VAE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Use the trained convolutional encoder to generate latent space representations\n",
        "# for the spectrogram data. The encoder outputs z_mean, z_log_var, and z (sampled latent vector).\n",
        "# We typically use z_mean as the deterministic latent feature vector for downstream tasks.\n",
        "\n",
        "# The `predict` method of the Keras model will run the input data through the `conv_encoder`\n",
        "# and return its outputs (z_mean, z_log_var, z).\n",
        "z_mean_spec, z_log_var_spec, z_spec = conv_encoder.predict(\n",
        "    spec_data_normalized, # Input: the normalized spectrogram data\n",
        "    verbose=0             # Suppress verbose output during prediction\n",
        ")\n",
        "\n",
        "# We are primarily interested in the mean of the latent distribution (z_mean)\n",
        "# as it provides a stable and representative embedding of the input spectrogram.\n",
        "spectrogram_latent_features = z_mean_spec\n",
        "\n",
        "print(f\"Spectrogram latent features (z_mean) extracted. Shape: {spectrogram_latent_features.shape}\")\n",
        "\n",
        "# You can also choose to use the sampled latent vector 'z_spec' or a combination\n",
        "# if your application requires stochasticity or further investigation of the latent space.\n",
        "# For most clustering or classification tasks, z_mean is preferred."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0dPahRHcAwMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdc6f875"
      },
      "source": [
        "# ========================================\n",
        "# CELL 10: Hybrid VAE Compilation and Summary\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"COMPILING HYBRID VAE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Instantiate the HybridVAE model\n",
        "# This creates an instance of our custom HybridVAE class, bringing together\n",
        "# the hybrid encoder and hybrid decoder defined in CELL 9.\n",
        "hybrid_vae = HybridVAE(encoder=hybrid_encoder, decoder=hybrid_decoder)\n",
        "\n",
        "# Compile the model.\n",
        "# For the HybridVAE, the loss calculations (reconstruction losses for both\n",
        "# spectrograms and lyrics, plus KL divergence) are handled internally within\n",
        "# the overridden `train_step` and `test_step` methods. Therefore, we only\n",
        "# need to specify an optimizer here.\n",
        "# The Adam optimizer is chosen with a small learning rate for stable training.\n",
        "hybrid_vae.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-4))\n",
        "\n",
        "print(\"\\nHybrid VAE Model Summary:\")\n",
        "# To display a summary of the entire Hybrid VAE model, it needs to be built first.\n",
        "# The `build` method requires sample input shapes for all its inputs.\n",
        "# The HybridVAE expects two inputs: spectrogram data and lyrics embeddings.\n",
        "# input_shape was determined in CELL 5, and lyrics_dim in CELL 9.\n",
        "hybrid_vae.build(input_shape=[(None,) + input_shape, (None, lyrics_dim)])\n",
        "hybrid_vae.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d3da2be"
      },
      "source": [
        "# ========================================\n",
        "# CELL 11: Train Hybrid VAE\n",
        "# ========================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TRAINING HYBRID VAE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Define base path for saving results, assuming it's the same as data_path\n",
        "# This variable should be defined in a preceding cell (e.g., CELL 4).\n",
        "base_path = '/content/drive/MyDrive/VAE for Hybrid Language Music Clustering'\n",
        "\n",
        "# Train the Hybrid VAE model.\n",
        "# The `fit` method receives a list of inputs (spectrograms and lyrics)\n",
        "# and a list of targets (which are the same as the inputs for a VAE).\n",
        "hybrid_history = hybrid_vae.fit(\n",
        "    [spec_data_normalized, lyrics_embeddings_normalized], # List of input data for each modality\n",
        "    [spec_data_normalized, lyrics_embeddings_normalized], # List of target data for reconstruction\n",
        "    epochs=50,               # Number of times to iterate over the entire dataset\n",
        "    batch_size=32,           # Number of samples per gradient update\n",
        "    validation_split=0.2,    # Fraction of the training data to be used as validation data\n",
        "    verbose=1                # Display progress bar during training\n",
        ")\n",
        "\n",
        "# --- Plotting Training History ---\n",
        "# Visualize the training and validation loss over epochs to monitor model performance\n",
        "# and identify potential overfitting or underfitting. The HybridVAE tracks losses\n",
        "# for each modality and a total loss.\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot total training loss\n",
        "plt.plot(hybrid_history.history['loss'], label='Total Training Loss', linewidth=2)\n",
        "# Plot total validation loss\n",
        "plt.plot(hybrid_history.history['val_loss'], label='Total Validation Loss', linewidth=2)\n",
        "\n",
        "# Optionally, plot individual modality losses if desired\n",
        "# plt.plot(hybrid_history.history['reconstruction_spec_loss'], label='Spec. Recon. Loss (Train)', linestyle='--')\n",
        "# plt.plot(hybrid_history.history['val_reconstruction_spec_loss'], label='Spec. Recon. Loss (Val)', linestyle='--')\n",
        "# plt.plot(hybrid_history.history['reconstruction_lyrics_loss'], label='Lyrics Recon. Loss (Train)', linestyle=':')\n",
        "# plt.plot(hybrid_history.history['val_reconstruction_lyrics_loss'], label='Lyrics Recon. Loss (Val)', linestyle=':')\n",
        "# plt.plot(hybrid_history.history['kl_loss'], label='KL Loss (Train)', linestyle='-.')\n",
        "# plt.plot(hybrid_history.history['val_kl_loss'], label='KL Loss (Val)', linestyle='-.')\n",
        "\n",
        "# Add labels and title for clarity\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Loss', fontsize=12)\n",
        "plt.title('Hybrid VAE Training History', fontsize=14)\n",
        "\n",
        "# Add a legend to distinguish between training and validation losses\n",
        "plt.legend(fontsize=10)\n",
        "\n",
        "# Add a grid for better readability of data points\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Ensure the results directory exists before saving\n",
        "results_dir = os.path.join(base_path, 'results/visualizations')\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "# Save the plot to a file\n",
        "plt.savefig(os.path.join(results_dir, 'hybrid_vae_training.png'), dpi=300, bbox_inches='tight')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================\n",
        "# CELL 11: Extract Hybrid Latent Features\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EXTRACTING HYBRID LATENT FEATURES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Use the trained hybrid encoder to generate latent space representations\n",
        "# for the combined spectrogram and lyrics data.\n",
        "# The `hybrid_encoder` outputs z_mean, z_log_var, and z (sampled latent vector).\n",
        "# We typically use z_mean as the deterministic latent feature vector for downstream tasks.\n",
        "\n",
        "# The `predict` method of the Keras model will run the input data through the `hybrid_encoder`\n",
        "# and return its outputs (z_mean_hybrid, z_log_var_hybrid, hybrid_z_sampled).\n",
        "z_mean_hybrid, z_log_var_hybrid, hybrid_z_sampled = hybrid_encoder.predict(\n",
        "    [\n",
        "        spec_data_normalized,       # Input: the normalized spectrogram data\n",
        "        lyrics_embeddings_normalized  # Input: the normalized lyrics embeddings\n",
        "    ],\n",
        "    verbose=0                     # Suppress verbose output during prediction\n",
        ")\n",
        "\n",
        "# We are primarily interested in the mean of the latent distribution (z_mean_hybrid)\n",
        "# as it provides a stable and representative embedding of the combined audio and lyrics input.\n",
        "hybrid_latent_features = z_mean_hybrid\n",
        "\n",
        "print(f\"Hybrid latent features (z_mean) extracted. Shape: {hybrid_latent_features.shape}\")\n",
        "\n",
        "# These `hybrid_latent_features` can now be used for downstream tasks such as\n",
        "# clustering, classification, or visualization, representing a fused understanding\n",
        "# of both audio and lyrical content."
      ],
      "metadata": {
        "id": "pmmdM0L2USQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "bu2jldQ9WAFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b22e8ab6"
      },
      "source": [
        "# ========================================\n",
        "# CELL 12: Clustering with Multiple Algorithms\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CLUSTERING WITH MULTIPLE ALGORITHMS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Define the desired number of clusters for algorithms like KMeans and Agglomerative Clustering.\n",
        "# This value might need to be adjusted based on domain knowledge or further analysis (e.g., elbow method, silhouette analysis).\n",
        "n_clusters = 5\n",
        "\n",
        "# Initialize 'true_labels'.\n",
        "# This variable is crucial for evaluating clustering performance using metrics like Adjusted Rand Index (ARI).\n",
        "# If actual ground truth labels are not available or are mismatched in length, ARI cannot be computed, or\n",
        "# it could lead to an 'inconsistent length' error if attempted with incorrect data.\n",
        "# The 'true_labels' are assumed to be loaded from the dataset in an earlier cell (e.g., CELL 2).\n",
        "# If true_labels is None, ARI calculation will be skipped.\n",
        "\n",
        "# Prepare different feature sets for clustering.\n",
        "# These feature sets are the latent representations learned by our VAE models.\n",
        "feature_sets = {\n",
        "    'Conv-VAE': spectrogram_latent_features, # Latent features from the Convolutional VAE (audio only)\n",
        "    'Hybrid-VAE': hybrid_latent_features     # Latent features from the Hybrid VAE (audio + lyrics)\n",
        "}\n",
        "\n",
        "# Define a dictionary of clustering algorithms to be applied.\n",
        "# Each algorithm is instantiated with its parameters.\n",
        "clustering_algorithms = {\n",
        "    'K-Means': KMeans(n_clusters=n_clusters, random_state=42, n_init=10), # KMeans for partitioning data into k clusters\n",
        "    'Agglomerative': AgglomerativeClustering(n_clusters=n_clusters), # Hierarchical clustering (bottom-up approach)\n",
        "    'DBSCAN': DBSCAN(eps=0.5, min_samples=5) # Density-based spatial clustering of applications with noise\n",
        "}\n",
        "\n",
        "# List to store the results (metrics) for each combination of feature set and algorithm.\n",
        "results = []\n",
        "\n",
        "# Iterate through each feature set.\n",
        "for feat_name, features in feature_sets.items():\n",
        "    print(f\"\\n{'='*40}\")\n",
        "    print(f\"Feature Set: {feat_name}\")\n",
        "    print(f\"{'='*40}\")\n",
        "\n",
        "    # Iterate through each clustering algorithm.\n",
        "    for algo_name, algo in clustering_algorithms.items():\n",
        "        print(f\"\\n  Algorithm: {algo_name}\")\n",
        "\n",
        "        # Perform clustering based on the algorithm type.\n",
        "        if algo_name == 'DBSCAN':\n",
        "            # DBSCAN does not require n_clusters upfront; it discovers clusters based on density.\n",
        "            clusters = algo.fit_predict(features)\n",
        "            # Count unique clusters found by DBSCAN, excluding noise points (-1).\n",
        "            n_clusters_found = len(set(clusters)) - (1 if -1 in clusters else 0)\n",
        "            print(f\"  Clusters found by DBSCAN: {n_clusters_found} (Noise points detected: {-1 in clusters})\")\n",
        "        else:\n",
        "            # For K-Means and Agglomerative, fit and predict clusters.\n",
        "            clusters = algo.fit_predict(features)\n",
        "\n",
        "        # Check for sufficient clusters to compute evaluation metrics.\n",
        "        # Metrics like Silhouette Score require at least 2 clusters.\n",
        "        if len(np.unique(clusters)) < 2:\n",
        "            print(f\"  Warning: Only {len(np.unique(clusters))} effective cluster found. Skipping internal validity metrics for this run.\")\n",
        "\n",
        "            # Store partial results, indicating skipped metrics\n",
        "            results.append({\n",
        "                'Feature_Set': feat_name,\n",
        "                'Algorithm': algo_name,\n",
        "                'Silhouette_Score': 'N/A',\n",
        "                'Davies_Bouldin_Index': 'N/A',\n",
        "                'Calinski_Harabasz_Index': 'N/A',\n",
        "                'ARI': 'N/A',\n",
        "                'N_Clusters': len(np.unique(clusters))\n",
        "            })\n",
        "            continue # Skip to the next algorithm if not enough clusters\n",
        "\n",
        "        # --- Calculate and print clustering evaluation metrics ---\n",
        "        # 1. Silhouette Score: Measures how similar an object is to its own cluster (cohesion)\n",
        "        #    compared to other clusters (separation). Higher is better (range -1 to 1).\n",
        "        silhouette = silhouette_score(features, clusters)\n",
        "\n",
        "        # 2. Davies-Bouldin Index: Measures the average similarity ratio of each cluster\n",
        "        #    with its most similar cluster. Lower is better (0 to infinity).\n",
        "        davies_bouldin = davies_bouldin_score(features, clusters)\n",
        "\n",
        "        # 3. Calinski-Harabasz Index (Variance Ratio Criterion): Measures the ratio of\n",
        "        #    between-cluster dispersion to within-cluster dispersion. Higher is better (0 to infinity).\n",
        "        calinski = calinski_harabasz_score(features, clusters)\n",
        "\n",
        "        # 4. Adjusted Rand Index (ARI): Measures the similarity of the clustering to the\n",
        "        #    ground truth labels, adjusting for chance. Requires true_labels. Higher is better (range -1 to 1).\n",
        "        #    Only compute if true_labels are available and have consistent length.\n",
        "        ari_score = None\n",
        "        if true_labels is not None and len(true_labels) == len(clusters):\n",
        "            ari_score = adjusted_rand_score(true_labels, clusters)\n",
        "            print(f\"  Adjusted Rand Index (ARI): {ari_score:.4f}\")\n",
        "        else:\n",
        "            print(\"  Adjusted Rand Index (ARI): N/A (True labels not available or mismatched length)\")\n",
        "\n",
        "        print(f\"  Silhouette Score: {silhouette:.4f}\")\n",
        "        print(f\"  Davies-Bouldin Index: {davies_bouldin:.4f}\")\n",
        "        print(f\"  Calinski-Harabasz Index: {calinski:.4f}\")\n",
        "\n",
        "        # Store the results of the current clustering run.\n",
        "        results.append({\n",
        "            'Feature_Set': feat_name,\n",
        "            'Algorithm': algo_name,\n",
        "            'Silhouette_Score': silhouette,\n",
        "            'Davies_Bouldin_Index': davies_bouldin,\n",
        "            'Calinski_Harabasz_Index': calinski,\n",
        "            'ARI': ari_score if ari_score is not None else 'N/A',\n",
        "            'N_Clusters': len(np.unique(clusters)) # Number of unique clusters found\n",
        "        })\n",
        "\n",
        "# Convert results to a DataFrame for better viewing and further analysis.\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CLUSTERING RESULTS SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "display(results_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZwAzR85YWY1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47919581"
      },
      "source": [
        "# ========================================\n",
        "# CELL 13: Save and Display Results\n",
        "# ========================================\n",
        "\n",
        "# Ensure the results_df DataFrame is created from the 'results' list\n",
        "# (This DataFrame was already created and displayed in CELL 12, but it's good practice\n",
        "# to explicitly redefine or ensure its existence if this cell were run independently).\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CLUSTERING RESULTS SUMMARY (Detailed View)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Display the DataFrame containing all clustering results.\n",
        "# Using display() for better formatting in Colab notebooks.\n",
        "display(results_df)\n",
        "\n",
        "# --- Save Results to CSV ---\n",
        "# Define the directory for saving metrics.\n",
        "metrics_dir = os.path.join(base_path, 'results/metrics')\n",
        "# Create the directory if it does not already exist.\n",
        "os.makedirs(metrics_dir, exist_ok=True)\n",
        "\n",
        "# Define the full path for the CSV file.\n",
        "output_csv_path = os.path.join(metrics_dir, 'comprehensive_clustering_metrics.csv')\n",
        "\n",
        "# Save the results DataFrame to a CSV file without including the DataFrame index.\n",
        "results_df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "print(f\"\\nResults saved to CSV: {output_csv_path}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YGDJCUbXW4dY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f55bafd"
      },
      "source": [
        "# ========================================\n",
        "# CELL 14: Visualize Metrics Comparison\n",
        "# ========================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd # Ensure pandas is imported if not already\n",
        "import os           # Ensure os is imported if not already\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"VISUALIZING CLUSTERING METRICS COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create a figure and a 2x2 grid of subplots for the visualizations.\n",
        "# This allows for a side-by-side comparison of different metrics.\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# --- Plotting Silhouette Score ---\n",
        "# The Silhouette Score measures how similar an object is to its own cluster (cohesion)\n",
        "# compared to other clusters (separation). Higher values are better (range -1 to 1).\n",
        "# Pivot the results DataFrame to easily plot scores for each Feature_Set and Algorithm.\n",
        "pivot_sil = results_df.pivot(index='Feature_Set', columns='Algorithm', values='Silhouette_Score')\n",
        "# Generate a bar plot on the first subplot (top-left).\n",
        "pivot_sil.plot(kind='bar', ax=axes[0, 0], rot=0) # rot=0 prevents x-axis labels from rotating\n",
        "axes[0, 0].set_title('Silhouette Score (Higher is Better)', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].set_ylabel('Score', fontsize=12)\n",
        "axes[0, 0].legend(title='Algorithm', fontsize=10)\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# --- Plotting Davies-Bouldin Index ---\n",
        "# The Davies-Bouldin Index measures the average similarity ratio of each cluster\n",
        "# with its most similar cluster. Lower values are better (0 to infinity).\n",
        "pivot_db = results_df.pivot(index='Feature_Set', columns='Algorithm', values='Davies_Bouldin_Index')\n",
        "# Generate a bar plot on the second subplot (top-right) with custom colors.\n",
        "pivot_db.plot(kind='bar', ax=axes[0, 1], rot=0, color=['#ff9999', '#66b3ff', '#99ff99'])\n",
        "axes[0, 1].set_title('Davies-Bouldin Index (Lower is Better)', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].set_ylabel('Score', fontsize=12)\n",
        "axes[0, 1].legend(title='Algorithm', fontsize=10)\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# --- Plotting Calinski-Harabasz Index ---\n",
        "# The Calinski-Harabasz Index (Variance Ratio Criterion) measures the ratio of\n",
        "# between-cluster dispersion to within-cluster dispersion. Higher values are better (0 to infinity).\n",
        "pivot_ch = results_df.pivot(index='Feature_Set', columns='Algorithm', values='Calinski_Harabasz_Index')\n",
        "# Generate a bar plot on the third subplot (bottom-left) with custom colors.\n",
        "pivot_ch.plot(kind='bar', ax=axes[1, 0], rot=0, color=['#ffcc99', '#ff99cc', '#99ccff'])\n",
        "axes[1, 0].set_title('Calinski-Harabasz Index (Higher is Better)', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].set_ylabel('Score', fontsize=12)\n",
        "axes[1, 0].legend(title='Algorithm', fontsize=10)\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# --- Plotting Adjusted Rand Index (ARI) ---\n",
        "# ARI measures the similarity of the clustering to the ground truth labels. Higher values are better.\n",
        "# This plot is conditional, as ARI requires ground truth labels which might not always be available.\n",
        "\n",
        "# Check if ARI values are numeric (not 'N/A' strings) before plotting.\n",
        "# If 'N/A' values exist, they are objects, otherwise, they would be numeric (float, int).\n",
        "if results_df['ARI'].dtype != 'object':\n",
        "    pivot_ari = results_df.pivot(index='Feature_Set', columns='Algorithm', values='ARI')\n",
        "    # Generate a bar plot on the fourth subplot (bottom-right) with custom colors.\n",
        "    pivot_ari.plot(kind='bar', ax=axes[1, 1], rot=0, color=['#c2c2f0', '#ffb3e6', '#c2f0c2'])\n",
        "    axes[1, 1].set_title('Adjusted Rand Index (Higher is Better)', fontsize=14, fontweight='bold')\n",
        "    axes[1, 1].set_ylabel('Score', fontsize=12)\n",
        "    axes[1, 1].legend(title='Algorithm', fontsize=10)\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "else:\n",
        "    # If ARI is not available (due to missing or mismatched ground truth labels),\n",
        "    # display a message in the subplot and turn off its axis.\n",
        "    axes[1, 1].text(0.5, 0.5, 'ARI Not Available\\n(No Ground Truth Labels)',\n",
        "                    ha='center', va='center', fontsize=14, transform=axes[1, 1].transAxes)\n",
        "    axes[1, 1].axis('off')\n",
        "\n",
        "# Adjust layout to prevent overlapping titles/labels.\n",
        "plt.tight_layout()\n",
        "\n",
        "# Ensure the results/visualizations directory exists before saving.\n",
        "results_dir = os.path.join(base_path, 'results/visualizations')\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "# Save the figure to a file.\n",
        "plt.savefig(os.path.join(results_dir, 'metrics_comparison.png'), dpi=300, bbox_inches='tight')\n",
        "\n",
        "# Display the plot.\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qPu8QLDNXcpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32ee5e4d"
      },
      "source": [
        "# ========================================\n",
        "# CELL 16: UMAP Visualization\n",
        "# ========================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import umap # Ensure umap-learn is imported (from cell 1, so already available)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"GENERATING UMAP VISUALIZATIONS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# --- 1. Identify Best Performing Method for Each Feature Set ---\n",
        "# This section re-uses the 'best_methods' dictionary populated in CELL 15,\n",
        "# which contains the best clustering algorithm (based on Silhouette Score)\n",
        "# for each feature set (Conv-VAE and Hybrid-VAE).\n",
        "# This ensures consistency in visualizing the top performers.\n",
        "\n",
        "# Ensure 'best_methods' is available. If CELL 15 failed, this might be empty.\n",
        "if not best_methods:\n",
        "    print(\"Warning: 'best_methods' not populated. Re-running logic to find best methods...\")\n",
        "    best_methods = {}\n",
        "    for feat_name in feature_sets.keys():\n",
        "        feat_results = results_df[results_df['Feature_Set'] == feat_name]\n",
        "        feat_results_numeric_sil = pd.to_numeric(feat_results['Silhouette_Score'], errors='coerce')\n",
        "        if not feat_results_numeric_sil.dropna().empty:\n",
        "            best_idx = feat_results_numeric_sil.idxmax()\n",
        "            best_methods[feat_name] = feat_results.loc[best_idx]\n",
        "        else:\n",
        "            best_methods[feat_name] = None\n",
        "    best_methods = {k: v for k, v in best_methods.items() if v is not None}\n",
        "\n",
        "if not best_methods:\n",
        "    print(\"No best methods could be identified for UMAP visualization. Skipping UMAP plots.\")\n",
        "else:\n",
        "    # --- 2. Initialize Plotting Area ---\n",
        "    # Create a figure with a subplot for each best method.\n",
        "    # The number of subplots depends on how many feature sets had a 'best' method.\n",
        "    fig, axes = plt.subplots(1, len(best_methods), figsize=(10 * len(best_methods), 6))\n",
        "    if len(best_methods) == 1: # Adjust for single plot case\n",
        "        axes = [axes]\n",
        "\n",
        "    print(\"\\nApplying UMAP and generating visualizations...\")\n",
        "    # --- 3. Generate UMAP Plots for Best Methods ---\n",
        "    # Iterate through each identified best method to create a UMAP visualization.\n",
        "    for idx, (feat_name, best_method_row) in enumerate(best_methods.items()):\n",
        "        features = feature_sets[feat_name] # Get the original high-dimensional features.\n",
        "        best_algo_name = best_method_row['Algorithm']\n",
        "        silhouette_score_val = best_method_row['Silhouette_Score']\n",
        "\n",
        "        print(f\"  Processing {feat_name} with {best_algo_name} (Silhouette: {silhouette_score_val:.3f})...\")\n",
        "\n",
        "        # Recreate the clustering with the best identified algorithm.\n",
        "        # This ensures the UMAP visualization reflects the clusters found by that specific algorithm.\n",
        "        if best_algo_name == 'K-Means':\n",
        "            algo = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "        elif best_algo_name == 'Agglomerative':\n",
        "            algo = AgglomerativeClustering(n_clusters=n_clusters)\n",
        "        elif best_algo_name == 'DBSCAN':\n",
        "            algo = DBSCAN(eps=0.5, min_samples=5)\n",
        "        else:\n",
        "            print(f\"  Warning: Unrecognized algorithm {best_algo_name} for {feat_name}. Skipping UMAP plot.\")\n",
        "            axes[idx].text(0.5, 0.5, f'Unrecognized Algo for UMAP ({feat_name})', ha='center', va='center', fontsize=12)\n",
        "            axes[idx].axis('off')\n",
        "            continue\n",
        "\n",
        "        clusters = algo.fit_predict(features)\n",
        "\n",
        "        # Apply UMAP (Uniform Manifold Approximation and Projection).\n",
        "        # UMAP is another dimensionality reduction technique, often better at preserving\n",
        "        # the global structure of the data compared to t-SNE, while still revealing local structure.\n",
        "        # 'n_neighbors' and 'min_dist' are important hyperparameters that can be tuned.\n",
        "        reducer = umap.UMAP(random_state=42, n_neighbors=15, min_dist=0.1)\n",
        "        umap_features = reducer.fit_transform(features)\n",
        "\n",
        "        # Plot the UMAP results, coloring points by their assigned cluster.\n",
        "        scatter = axes[idx].scatter(\n",
        "            umap_features[:, 0], umap_features[:, 1], # X and Y coordinates from UMAP\n",
        "            c=clusters,                          # Color points based on cluster assignments\n",
        "            cmap='tab10',                        # Colormap for clusters\n",
        "            alpha=0.6,                           # Transparency of points\n",
        "            s=50                                 # Size of points\n",
        "        )\n",
        "        axes[idx].set_title(\n",
        "            f'{feat_name} + {best_algo_name}\\n(Silhouette: {silhouette_score_val:.3f})',\n",
        "            fontsize=12,\n",
        "            fontweight='bold'\n",
        "        )\n",
        "        axes[idx].set_xlabel('UMAP Dimension 1', fontsize=11)\n",
        "        axes[idx].set_ylabel('UMAP Dimension 2', fontsize=11)\n",
        "        plt.colorbar(scatter, ax=axes[idx], label='Cluster')\n",
        "        axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "    # --- 4. Final Plot Adjustments and Saving ---\n",
        "    plt.tight_layout() # Adjust layout to prevent overlapping elements.\n",
        "\n",
        "    # Ensure the results directory exists before saving.\n",
        "    results_dir = os.path.join(base_path, 'results/visualizations')\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "    # Save the generated UMAP plot to a file.\n",
        "    plt.savefig(os.path.join(results_dir, 'umap_best_methods.png'), dpi=300, bbox_inches='tight')\n",
        "\n",
        "    # Display the plot.\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ec484f4"
      },
      "source": [
        "# ========================================\n",
        "# CELL 15: t-SNE Visualization for Best Methods\n",
        "# ========================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.manifold import TSNE # Ensure TSNE is imported\n",
        "import numpy as np                # Ensure numpy is imported\n",
        "import pandas as pd               # Ensure pandas is imported\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"GENERATING t-SNE VISUALIZATIONS FOR BEST CLUSTERING METHODS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# --- 1. Identify Best Performing Method for Each Feature Set ---\n",
        "# This section determines which clustering algorithm performed best (highest Silhouette Score)\n",
        "# for each feature set (Conv-VAE and Hybrid-VAE) based on the previously computed results.\n",
        "best_methods = {}\n",
        "for feat_name in feature_sets.keys():\n",
        "    # Filter results for the current feature set.\n",
        "    feat_results = results_df[results_df['Feature_Set'] == feat_name]\n",
        "\n",
        "    # Ensure Silhouette Scores are numeric before finding the maximum.\n",
        "    # If 'N/A' is present due to DBSCAN, convert it to NaN for proper comparison.\n",
        "    feat_results_numeric_sil = pd.to_numeric(feat_results['Silhouette_Score'], errors='coerce')\n",
        "\n",
        "    # Find the index of the row with the maximum Silhouette Score (excluding NaN values).\n",
        "    # If all are NaN (e.g., for DBSCAN with no clusters), handle gracefully.\n",
        "    if not feat_results_numeric_sil.dropna().empty:\n",
        "        best_idx = feat_results_numeric_sil.idxmax()\n",
        "        best_methods[feat_name] = feat_results.loc[best_idx] # Store the full row for the best method.\n",
        "    else:\n",
        "        # Fallback if no valid silhouette scores were found (e.g., only DBSCAN with 1 cluster).\n",
        "        print(f\"  Warning: No valid Silhouette Score found for {feat_name}. Skipping selection of best method.\")\n",
        "        best_methods[feat_name] = None # Mark as None or handle as appropriate.\n",
        "\n",
        "# Filter out feature sets that couldn't determine a best method\n",
        "best_methods = {k: v for k, v in best_methods.items() if v is not None}\n",
        "\n",
        "if not best_methods:\n",
        "    print(\"No best methods could be identified for t-SNE visualization due to lack of valid clustering results.\")\n",
        "else:\n",
        "    # --- 2. Initialize Plotting Area ---\n",
        "    # Create a figure with a subplot for each best method (typically 2 plots: Conv-VAE and Hybrid-VAE).\n",
        "    fig, axes = plt.subplots(1, len(best_methods), figsize=(10 * len(best_methods), 6))\n",
        "    if len(best_methods) == 1:\n",
        "        axes = [axes] # Ensure axes is iterable even for a single subplot.\n",
        "\n",
        "    print(\"\\nApplying t-SNE and generating visualizations...\")\n",
        "    # --- 3. Generate t-SNE Plots for Best Methods ---\n",
        "    for idx, (feat_name, best_method_row) in enumerate(best_methods.items()):\n",
        "        features = feature_sets[feat_name] # Get the original features for this set.\n",
        "        best_algo_name = best_method_row['Algorithm']\n",
        "        silhouette_score_val = best_method_row['Silhouette_Score']\n",
        "\n",
        "        print(f\"  Processing {feat_name} with {best_algo_name} (Silhouette: {silhouette_score_val:.3f})...\")\n",
        "\n",
        "        # Recreate the clustering with the best identified algorithm.\n",
        "        # This ensures we plot the clusters as determined by that specific algorithm.\n",
        "        if best_algo_name == 'K-Means':\n",
        "            algo = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "        elif best_algo_name == 'Agglomerative':\n",
        "            algo = AgglomerativeClustering(n_clusters=n_clusters)\n",
        "        elif best_algo_name == 'DBSCAN': # Although DBSCAN often results in few clusters with default params\n",
        "            algo = DBSCAN(eps=0.5, min_samples=5)\n",
        "        else:\n",
        "            # This case should ideally not be reached if best_methods is populated correctly.\n",
        "            print(f\"  Warning: Unrecognized algorithm {best_algo_name} for {feat_name}. Skipping t-SNE.\")\n",
        "            continue\n",
        "\n",
        "        clusters = algo.fit_predict(features)\n",
        "\n",
        "        # Ensure enough data points for t-SNE perplexity requirement.\n",
        "        # perplexity must be less than the number of samples.\n",
        "        perplexity_val = min(30, len(features) - 1) if len(features) > 1 else 1\n",
        "        if perplexity_val <= 1: # t-SNE requires perplexity > 1\n",
        "            print(f\"  Warning: Not enough samples for t-SNE for {feat_name}. Skipping t-SNE.\")\n",
        "            axes[idx].text(0.5, 0.5, f'Not enough data for t-SNE ({feat_name})', ha='center', va='center', fontsize=12)\n",
        "            axes[idx].axis('off')\n",
        "            continue\n",
        "\n",
        "        # Apply t-SNE (t-Distributed Stochastic Neighbor Embedding).\n",
        "        # t-SNE reduces high-dimensional data to 2 or 3 dimensions for visualization,\n",
        "        # preserving local structures within the data.\n",
        "        tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_val)\n",
        "        tsne_features = tsne.fit_transform(features)\n",
        "\n",
        "        # Plot the t-SNE results, coloring points by their assigned cluster.\n",
        "        scatter = axes[idx].scatter(\n",
        "            tsne_features[:, 0], tsne_features[:, 1], # X and Y coordinates from t-SNE\n",
        "            c=clusters,                          # Color points based on cluster assignments\n",
        "            cmap='tab10',                        # Colormap for clusters\n",
        "            alpha=0.6,                           # Transparency of points\n",
        "            s=50                                 # Size of points\n",
        "        )\n",
        "        axes[idx].set_title(\n",
        "            f'{feat_name} + {best_algo_name}\\n(Silhouette: {silhouette_score_val:.3f})',\n",
        "            fontsize=12,\n",
        "            fontweight='bold'\n",
        "        )\n",
        "        axes[idx].set_xlabel('t-SNE Dimension 1', fontsize=11)\n",
        "        axes[idx].set_ylabel('t-SNE Dimension 2', fontsize=11)\n",
        "        plt.colorbar(scatter, ax=axes[idx], label='Cluster')\n",
        "        axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "    # --- 4. Final Plot Adjustments and Saving ---\n",
        "    plt.tight_layout() # Adjust layout to prevent overlapping elements.\n",
        "\n",
        "    # Ensure the results directory exists before saving.\n",
        "    results_dir = os.path.join(base_path, 'results/visualizations')\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "    # Save the generated t-SNE plot to a file.\n",
        "    plt.savefig(os.path.join(results_dir, 'tsne_best_methods.png'), dpi=300, bbox_inches='tight')\n",
        "\n",
        "    # Display the plot.\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1be6d94"
      },
      "source": [
        "# ========================================\n",
        "# CELL 17: Analysis and Insights\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ANALYSIS AND INSIGHTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# --- 1. Identify Overall Best Performing Method ---\n",
        "# This section identifies the clustering algorithm and feature set combination\n",
        "# that yielded the highest Silhouette Score, which is generally indicative\n",
        "# of well-separated and cohesive clusters.\n",
        "\n",
        "# Ensure 'Silhouette_Score' column is numeric, coercing any 'N/A' values to NaN for max() operation.\n",
        "results_df_numeric_sil = results_df.copy()\n",
        "results_df_numeric_sil['Silhouette_Score'] = pd.to_numeric(results_df_numeric_sil['Silhouette_Score'], errors='coerce')\n",
        "\n",
        "if not results_df_numeric_sil['Silhouette_Score'].dropna().empty:\n",
        "    best_overall = results_df_numeric_sil.loc[results_df_numeric_sil['Silhouette_Score'].idxmax()]\n",
        "    print(f\"\\nOverall Best Method (by Silhouette Score):\")\n",
        "    print(f\"  Feature Set: {best_overall['Feature_Set']}\")\n",
        "    print(f\"  Algorithm: {best_overall['Algorithm']}\")\n",
        "    print(f\"  Silhouette Score: {best_overall['Silhouette_Score']:.4f}\")\n",
        "    # Display Davies-Bouldin and Calinski-Harabasz if available and numeric\n",
        "    if pd.isna(best_overall['Davies_Bouldin_Index']): # Check if N/A from DBSCAN case\n",
        "        print(f\"  Davies-Bouldin Index: N/A\")\n",
        "    else:\n",
        "        print(f\"  Davies-Bouldin Index: {best_overall['Davies_Bouldin_Index']:.4f}\")\n",
        "    if pd.isna(best_overall['Calinski_Harabasz_Index']): # Check if N/A from DBSCAN case\n",
        "        print(f\"  Calinski-Harabasz Index: N/A\")\n",
        "    else:\n",
        "        print(f\"  Calinski-Harabasz Index: {best_overall['Calinski_Harabasz_Index']:.4f}\")\n",
        "    if best_overall['ARI'] != 'N/A':\n",
        "        print(f\"  Adjusted Rand Index: {best_overall['ARI']:.4f}\")\n",
        "else:\n",
        "    print(\"\\nCould not determine an overall best method as no valid Silhouette Scores were computed.\")\n",
        "\n",
        "# --- 2. Compare Performance Between Feature Sets (Conv-VAE vs. Hybrid-VAE) ---\n",
        "# This section provides a comparative overview of how different feature sets\n",
        "# (audio-only vs. multimodal) perform on average across the successful clustering algorithms.\n",
        "\n",
        "print(f\"\\n{'='*40}\")\n",
        "print(\"FEATURE SET COMPARISON (Average Metrics)\")\n",
        "print(f\"{'='*40}\")\n",
        "\n",
        "# Convert score columns to numeric, handling 'N/A' values as NaN.\n",
        "results_df_for_avg = results_df.copy()\n",
        "for col in ['Silhouette_Score', 'Davies_Bouldin_Index', 'Calinski_Harabasz_Index', 'ARI']:\n",
        "    results_df_for_avg[col] = pd.to_numeric(results_df_for_avg[col], errors='coerce')\n",
        "\n",
        "conv_vae_methods = results_df_for_avg[results_df_for_avg['Feature_Set'] == 'Conv-VAE']\n",
        "hybrid_vae_methods = results_df_for_avg[results_df_for_avg['Feature_Set'] == 'Hybrid-VAE']\n",
        "\n",
        "# Ensure there are valid numeric results before computing means\n",
        "if not conv_vae_methods['Silhouette_Score'].dropna().empty:\n",
        "    print(f\"  Conv-VAE based methods:\")\n",
        "    print(f\"    Avg Silhouette Score: {conv_vae_methods['Silhouette_Score'].mean():.4f}\")\n",
        "    print(f\"    Avg Davies-Bouldin Index: {conv_vae_methods['Davies_Bouldin_Index'].mean():.4f}\")\n",
        "    print(f\"    Avg Calinski-Harabasz Index: {conv_vae_methods['Calinski_Harabasz_Index'].mean():.4f}\")\n",
        "else:\n",
        "    print(\"  No valid numeric results for Conv-VAE based methods.\")\n",
        "\n",
        "if not hybrid_vae_methods['Silhouette_Score'].dropna().empty:\n",
        "    print(f\"  Hybrid-VAE based methods:\")\n",
        "    print(f\"    Avg Silhouette Score: {hybrid_vae_methods['Silhouette_Score'].mean():.4f}\")\n",
        "    print(f\"    Avg Davies-Bouldin Index: {hybrid_vae_methods['Davies_Bouldin_Index'].mean():.4f}\")\n",
        "    print(f\"    Avg Calinski-Harabasz Index: {hybrid_vae_methods['Calinski_Harabasz_Index'].mean():.4f}\")\n",
        "else:\n",
        "    print(\"  No valid numeric results for Hybrid-VAE based methods.\")\n",
        "\n",
        "# --- 3. Algorithm Performance Across All Features ---\n",
        "# This section aggregates the performance of each clustering algorithm\n",
        "# across all tested feature sets (Conv-VAE and Hybrid-VAE).\n",
        "\n",
        "print(f\"\\n{'='*40}\")\n",
        "print(f\"ALGORITHM PERFORMANCE (Average Across Feature Sets)\")\n",
        "print(f\"{'='*40}\")\n",
        "\n",
        "for algo_name in clustering_algorithms.keys():\n",
        "    algo_results = results_df_for_avg[results_df_for_avg['Algorithm'] == algo_name]\n",
        "\n",
        "    if not algo_results['Silhouette_Score'].dropna().empty:\n",
        "        print(f\"  {algo_name}:\")\n",
        "        print(f\"    Avg Silhouette Score: {algo_results['Silhouette_Score'].mean():.4f}\")\n",
        "        print(f\"    Avg Davies-Bouldin Index: {algo_results['Davies_Bouldin_Index'].mean():.4f}\")\n",
        "        print(f\"    Avg Calinski-Harabasz Index: {algo_results['Calinski_Harabasz_Index'].mean():.4f}\")\n",
        "    else:\n",
        "        print(f\"  {algo_name}: No valid numeric results for this algorithm.\")\n",
        "\n",
        "# --- 4. Key Insights (Manual Interpretation Guidance) ---\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"KEY INSIGHTS & NEXT STEPS (Manual Interpretation)\")\n",
        "print(f\"{'='*60}\")\n",
        "print(\"- Consider the 'Overall Best Method' as a strong candidate for further analysis.\")\n",
        "print(\"- Compare average metrics between Conv-VAE and Hybrid-VAE to understand the impact of multimodal fusion.\")\n",
        "print(\"- Analyze the average performance of each algorithm to identify generally robust methods.\")\n",
        "print(\"- The 'N/A' for ARI indicates the absence of ground truth labels for comparison. If you have true labels (e.g., genre), ensure they are loaded and passed to the clustering step for a more complete evaluation.\")\n",
        "print(\"- DBSCAN often requires careful tuning of 'eps' and 'min_samples'. If it found only 1 cluster, consider exploring different parameter values for it.\")\n",
        "print(\"- Review the UMAP and t-SNE visualizations to visually confirm the cluster separation and density.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H_3EfP1XZMTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8894bbe3"
      },
      "source": [
        "# ========================================\n",
        "# CELL 18: Why VAE Performs Better/Worse Analysis\n",
        "# ========================================\n",
        "\n",
        "import os\n",
        "import pandas as pd # Ensure pandas is imported for results_df\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"WHY VAE REPRESENTATIONS PERFORM BETTER/WORSE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# This section provides a qualitative analysis of why different VAE architectures\n",
        "# and clustering algorithms might perform better or worse based on their inherent\n",
        "# characteristics and typical use cases.\n",
        "analysis_text = \"\"\"\n",
        "ANALYSIS OF VAE vs BASELINE PERFORMANCE:\n",
        "(Qualitative Insights)\n",
        "\n",
        "1. CONVOLUTIONAL VAE (Audio-only Features):\n",
        "   Strengths:\n",
        "   - Excellent at capturing spatial and temporal patterns unique to spectrograms (e.g., textures, harmonic structures).\n",
        "   - Learns hierarchical representations, abstracting raw audio into meaningful features.\n",
        "   - Effective at dimensionality reduction while retaining crucial information for audio.\n",
        "\n",
        "   Weaknesses:\n",
        "   - Ignores semantic content of lyrics, potentially missing context relevant to genre or mood.\n",
        "   - Might struggle with highly diverse audio patterns if not extensively trained.\n",
        "\n",
        "2. HYBRID VAE (Audio + Lyrics Features):\n",
        "   Strengths:\n",
        "   - Leverages complementary information from two distinct modalities (audio and text).\n",
        "   - Can potentially learn richer, more semantically meaningful latent representations by fusing audio and lyrical context.\n",
        "   - May be more robust to noise or ambiguities present in a single modality alone.\n",
        "   - Particularly useful for tasks where both sound and meaning are important (e.g., genre classification, content-based recommendation).\n",
        "\n",
        "   Weaknesses:\n",
        "   - Increased model complexity and computational cost compared to single-modality VAEs.\n",
        "   - Requires careful balancing of modal contributions to prevent one modality from dominating the other.\n",
        "   - The quality of lyrics embeddings heavily influences the effectiveness of the text branch.\n",
        "\n",
        "3. PCA BASELINE (Note: PCA was not directly applied in this notebook for clustering but is a common baseline for feature comparison):\n",
        "   Strengths:\n",
        "   - Simple, fast, and computationally efficient.\n",
        "   - Provides a linear dimensionality reduction, identifying directions of maximum variance.\n",
        "\n",
        "   Weaknesses:\n",
        "   - Only captures linear relationships; highly non-linear data (like latent features from VAEs) might lose important structure.\n",
        "   - Not a generative model; cannot reconstruct original data or infer new data points.\n",
        "\n",
        "4. CLUSTERING ALGORITHM COMPARISON:\n",
        "   K-Means:\n",
        "   - Strengths: Relatively fast and efficient for large datasets. Works well with spherical, well-separated clusters.\n",
        "   - Weaknesses: Requires specifying the number of clusters in advance. Sensitive to initial centroid placement. Struggles with non-spherical or overlapping clusters.\n",
        "\n",
        "   Agglomerative Clustering:\n",
        "   - Strengths: Hierarchical output (dendrogram) provides insights into data structure. More flexible with cluster shapes than K-Means. Does not require specifying the number of clusters explicitly in its base form (though we set it for direct comparison).\n",
        "   - Weaknesses: Computationally more expensive, especially for large datasets. Can struggle with noisy data.\n",
        "\n",
        "   DBSCAN:\n",
        "   - Strengths: Can discover arbitrary-shaped clusters. Automatically detects noise points. Does not require specifying the number of clusters.\n",
        "   - Weaknesses: Highly sensitive to parameter selection (`eps` and `min_samples`). Struggles with varying density clusters or when clusters are not well-separated by density differences.\n",
        "\"\"\"\n",
        "\n",
        "# Print the qualitative analysis text.\n",
        "print(analysis_text)\n",
        "\n",
        "# --- Save Analysis and Numerical Results ---\n",
        "# Ensure the base_path variable is defined (e.g., from previous cells).\n",
        "base_path = '/content/drive/MyDrive/VAE for Hybrid Language Music Clustering'\n",
        "results_dir = os.path.join(base_path, 'results')\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "# Define the output file path for the analysis report.\n",
        "output_analysis_path = os.path.join(results_dir, 'analysis_report.txt')\n",
        "\n",
        "# Write the qualitative analysis and numerical results to the file.\n",
        "with open(output_analysis_path, 'w') as f:\n",
        "    f.write(analysis_text)\n",
        "    f.write(\"\\n\\nNUMERICAL RESULTS (Clustering Metrics):\\n\")\n",
        "    f.write(\"=\"*60 + \"\\n\")\n",
        "    # Convert the results_df DataFrame to a string for writing to the file.\n",
        "    f.write(results_df.to_string(index=False))\n",
        "\n",
        "print(f\"\\nFull analysis report (qualitative + numerical) saved to: {output_analysis_path}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c7591dc"
      },
      "source": [
        "# ========================================\n",
        "# CELL 19: Save Models and Latent Features\n",
        "# ========================================\n",
        "\n",
        "import os\n",
        "import numpy as np # Ensure numpy is imported\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SAVING MODELS AND LATENT FEATURES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Ensure the models directory exists before saving any models.\n",
        "models_dir = os.path.join(base_path, 'results/models')\n",
        "os.makedirs(models_dir, exist_ok=True)\n",
        "\n",
        "# --- Save Convolutional VAE Models ---\n",
        "# Save the entire Conv-VAE model, its encoder, and its decoder.\n",
        "# These models can be reloaded later for inference or fine-tuning.\n",
        "conv_vae.save(os.path.join(models_dir, 'conv_vae_model.h5'))\n",
        "conv_encoder.save(os.path.join(models_dir, 'conv_encoder_model.h5'))\n",
        "conv_decoder.save(os.path.join(models_dir, 'conv_decoder_model.h5'))\n",
        "print(\" Conv-VAE model, encoder, and decoder saved successfully.\")\n",
        "\n",
        "# --- Save Hybrid VAE Models ---\n",
        "# Save the entire Hybrid VAE model, its encoder, and its decoder.\n",
        "# These represent the multimodal learning architecture.\n",
        "hybrid_vae.save(os.path.join(models_dir, 'hybrid_vae_model.h5'))\n",
        "hybrid_encoder.save(os.path.join(models_dir, 'hybrid_encoder_model.h5'))\n",
        "hybrid_decoder.save(os.path.join(models_dir, 'hybrid_decoder_model.h5'))\n",
        "print(\" Hybrid-VAE model, encoder, and decoder saved successfully.\")\n",
        "\n",
        "# Ensure the latent features directory exists.\n",
        "latent_features_dir = os.path.join(base_path, 'results')\n",
        "os.makedirs(latent_features_dir, exist_ok=True)\n",
        "\n",
        "# --- Save Extracted Latent Features ---\n",
        "# Save the numpy arrays containing the extracted latent features from both models.\n",
        "# These can be loaded directly for downstream tasks without retraining the VAEs.\n",
        "np.save(os.path.join(latent_features_dir, 'latent_features_conv.npy'), spectrogram_latent_features)\n",
        "np.save(os.path.join(latent_features_dir, 'latent_features_hybrid.npy'), hybrid_latent_features)\n",
        "print(\" Spectrogram and Hybrid latent features saved successfully.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ba5546c"
      },
      "source": [
        "# ========================================\n",
        "# CELL 20: Reconstruction Quality Visualization\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"VISUALIZING RECONSTRUCTION QUALITY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# --- 1. Sample Data for Visualization ---\n",
        "# Select a small number of random samples from the normalized spectrogram data\n",
        "# to visually compare their original and reconstructed versions.\n",
        "n_samples_to_visualize = 5\n",
        "sample_indices = np.random.choice(len(spec_data_normalized), n_samples_to_visualize, replace=False)\n",
        "\n",
        "# --- 2. Perform Reconstruction using Conv-VAE ---\n",
        "# The Conv-VAE's `predict` method will encode the input spectrograms to their latent\n",
        "# representation and then decode them back to reconstruct the spectrograms.\n",
        "# We use the full `conv_vae` model here, which includes both encoder and decoder.\n",
        "# We only pass the selected original spectrograms for reconstruction.\n",
        "reconstructed_conv_samples = conv_vae.predict(spec_data_normalized[sample_indices], verbose=0)\n",
        "\n",
        "# --- 3. Plot Original vs. Reconstructed Spectrograms ---\n",
        "# Create a figure with two rows of subplots: one for original spectrograms and one for reconstructed.\n",
        "# The number of columns will correspond to the number of samples visualized.\n",
        "fig, axes = plt.subplots(2, n_samples_to_visualize, figsize=(20, 8))\n",
        "\n",
        "# Iterate through the sampled indices and plot both original and reconstructed spectrograms.\n",
        "for i, original_index in enumerate(sample_indices):\n",
        "    # Plot Original Spectrogram (top row)\n",
        "    # The [:, :, 0] is used to remove the channel dimension for imshow.\n",
        "    axes[0, i].imshow(spec_data_normalized[original_index, :, :, 0], aspect='auto', cmap='viridis')\n",
        "    axes[0, i].set_title(f'Original Sample {original_index}', fontsize=10)\n",
        "    axes[0, i].axis('off') # Hide axes ticks and labels for cleaner visual\n",
        "\n",
        "    # Plot Reconstructed Spectrogram (bottom row)\n",
        "    # The reconstructed_conv_samples[i] corresponds to the reconstruction of the i-th sampled image.\n",
        "    axes[1, i].imshow(reconstructed_conv_samples[i, :, :, 0], aspect='auto', cmap='viridis')\n",
        "    axes[1, i].set_title(f'Reconstructed Sample {original_index}', fontsize=10)\n",
        "    axes[1, i].axis('off') # Hide axes ticks and labels\n",
        "\n",
        "# Add labels to the rows for clarity.\n",
        "axes[0, 0].set_ylabel('Original Spectrogram', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_ylabel('Reconstructed Spectrogram', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Add a main title for the entire figure.\n",
        "plt.suptitle('Conv-VAE Reconstruction Quality: Original vs. Reconstructed Samples', fontsize=14, fontweight='bold', y=0.98)\n",
        "\n",
        "# Adjust subplot parameters for a tight layout.\n",
        "plt.tight_layout()\n",
        "\n",
        "# --- 4. Save and Display the Plot ---\n",
        "# Ensure the visualization directory exists.\n",
        "visualization_dir = os.path.join(base_path, 'results/visualizations')\n",
        "os.makedirs(visualization_dir, exist_ok=True)\n",
        "\n",
        "# Save the generated figure to a file.\n",
        "plt.savefig(os.path.join(visualization_dir, 'conv_vae_reconstruction_quality.png'), dpi=300, bbox_inches='tight')\n",
        "\n",
        "# Display the plot.\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LdQSnQBlbNPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1908d252"
      },
      "source": [
        "# ========================================\n",
        "# CELL 21: Latent Space Interpolation\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"LATENT SPACE INTERPOLATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# --- 1. Select Two Random Samples from the Latent Space ---\n",
        "# We pick two distinct indices to represent the start and end points of our interpolation.\n",
        "# z_mean_spec (spectrogram_latent_features) is the mean latent vector from the Conv-VAE encoder.\n",
        "idx1, idx2 = np.random.choice(len(z_mean_spec), 2, replace=False)\n",
        "latent_vector_1 = z_mean_spec[idx1]\n",
        "latent_vector_2 = z_mean_spec[idx2]\n",
        "\n",
        "# --- 2. Interpolate Linearly Between the Two Latent Vectors ---\n",
        "# Create a series of intermediate latent vectors by linearly interpolating between z1 and z2.\n",
        "# This demonstrates the smoothness of the learned latent space.\n",
        "n_interpolation_steps = 10 # Number of steps for the interpolation sequence\n",
        "interpolated_latent_vectors = np.zeros((n_interpolation_steps, latent_dim))\n",
        "\n",
        "for i in range(n_interpolation_steps):\n",
        "    alpha = i / (n_interpolation_steps - 1) # Alpha ranges from 0.0 to 1.0\n",
        "    # Linear interpolation formula: (1 - alpha) * start_vector + alpha * end_vector\n",
        "    interpolated_latent_vectors[i] = (1 - alpha) * latent_vector_1 + alpha * latent_vector_2\n",
        "\n",
        "# --- 3. Decode the Interpolated Latent Vectors Back into Spectrograms ---\n",
        "# Use the Conv-VAE's decoder to reconstruct spectrograms from each interpolated latent vector.\n",
        "# This shows how the reconstructed output smoothly transitions from sample 1 to sample 2.\n",
        "interpolated_spectrograms = conv_decoder.predict(interpolated_latent_vectors, verbose=0)\n",
        "\n",
        "# --- 4. Visualize the Interpolated Spectrograms ---\n",
        "# Plot the sequence of reconstructed spectrograms to observe the transition.\n",
        "fig, axes = plt.subplots(1, n_interpolation_steps, figsize=(25, 3))\n",
        "\n",
        "for i in range(n_interpolation_steps):\n",
        "    # Plot each interpolated spectrogram.\n",
        "    # The [:, :, 0] is used to remove the channel dimension for imshow.\n",
        "    axes[i].imshow(interpolated_spectrograms[i, :, :, 0], aspect='auto', cmap='viridis')\n",
        "    axes[i].set_title(f'Step {i+1}', fontsize=9)\n",
        "    axes[i].axis('off') # Hide axes ticks and labels for cleaner visual\n",
        "\n",
        "# Add a main title for the entire figure.\n",
        "plt.suptitle(f'Latent Space Interpolation: Spectrogram Transition (Sample {idx1} \\u2192 Sample {idx2})',\n",
        "             fontsize=14, fontweight='bold', y=1.02) # y adjusts title position\n",
        "\n",
        "# Adjust subplot parameters for a tight layout.\n",
        "plt.tight_layout()\n",
        "\n",
        "# --- 5. Save and Display the Plot ---\n",
        "# Ensure the visualization directory exists before saving.\n",
        "visualization_dir = os.path.join(base_path, 'results/visualizations')\n",
        "os.makedirs(visualization_dir, exist_ok=True)\n",
        "\n",
        "# Save the generated figure to a file.\n",
        "plt.savefig(os.path.join(visualization_dir, 'latent_interpolation_conv.png'), dpi=300, bbox_inches='tight')\n",
        "\n",
        "# Display the plot.\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b3eRocRrcACI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b5677a0"
      },
      "source": [
        "# ========================================\n",
        "# CELL 22: Cluster Distribution Analysis\n",
        "# ========================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd # Ensure pandas is imported\n",
        "import numpy as np # Ensure numpy is imported\n",
        "import os           # Ensure os is imported\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CLUSTER DISTRIBUTION ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# --- 1. Identify and Recreate Best Overall Clustering Method ---\n",
        "# This section identifies the clustering algorithm and feature set that performed best\n",
        "# based on the previously computed Silhouette Score, and then recreates the clustering.\n",
        "\n",
        "# Ensure 'best_overall' is available from CELL 17, which stores the best performing method.\n",
        "# We need to explicitly convert Silhouette_Score to numeric to use idxmax if it contains 'N/A'\n",
        "if 'best_overall' not in locals() or best_overall.empty: # Check if best_overall was not set or is empty\n",
        "    results_df_numeric_sil = results_df.copy()\n",
        "    results_df_numeric_sil['Silhouette_Score'] = pd.to_numeric(results_df_numeric_sil['Silhouette_Score'], errors='coerce')\n",
        "    if not results_df_numeric_sil['Silhouette_Score'].dropna().empty:\n",
        "        best_overall = results_df_numeric_sil.loc[results_df_numeric_sil['Silhouette_Score'].idxmax()]\n",
        "    else:\n",
        "        print(\"Warning: Could not determine 'best_overall' method. Please ensure CELL 17 ran successfully.\")\n",
        "        print(\"Skipping Cluster Distribution Analysis.\")\n",
        "        # Add a placeholder plot or exit\n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "        ax.text(0.5, 0.5, 'Analysis Skipped:\\nBest Method Not Found', ha='center', va='center', fontsize=16)\n",
        "        ax.axis('off')\n",
        "        plt.show()\n",
        "        exit() # Exit to prevent further errors\n",
        "\n",
        "best_feat_name = best_overall['Feature_Set']\n",
        "best_algo_name = best_overall['Algorithm']\n",
        "best_features = feature_sets[best_feat_name]\n",
        "\n",
        "# Recreate the clustering algorithm using the parameters of the best method.\n",
        "# This ensures consistency with the evaluation metrics.\n",
        "if best_algo_name == 'K-Means':\n",
        "    final_algo = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "elif best_algo_name == 'Agglomerative':\n",
        "    final_algo = AgglomerativeClustering(n_clusters=n_clusters)\n",
        "elif best_algo_name == 'DBSCAN':\n",
        "    # Note: If DBSCAN was selected as best, it likely means other algorithms had invalid scores.\n",
        "    # Using its original parameters from clustering_algorithms dict.\n",
        "    final_algo = DBSCAN(eps=0.5, min_samples=5) # Use default/tuned DBSCAN params\n",
        "else:\n",
        "    print(f\"Warning: Unrecognized best algorithm: {best_algo_name}. Defaulting to K-Means.\")\n",
        "    final_algo = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "\n",
        "# Apply the best clustering algorithm to the corresponding features.\n",
        "final_clusters = final_algo.fit_predict(best_features)\n",
        "\n",
        "print(f\"Clustering method: {best_feat_name} with {best_algo_name}\")\n",
        "print(f\"Number of clusters found: {len(np.unique(final_clusters))}\")\n",
        "\n",
        "# --- 2. Visualize Cluster Distribution ---\n",
        "# Create a figure with two subplots to show cluster sizes and, if available, relationship to true labels.\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Plot 1: Distribution of Samples per Cluster (Cluster Sizes)\n",
        "# This bar chart shows how many samples fall into each identified cluster.\n",
        "cluster_counts = pd.Series(final_clusters).value_counts().sort_index()\n",
        "axes[0].bar(cluster_counts.index, cluster_counts.values, color='skyblue', edgecolor='black')\n",
        "axes[0].set_xlabel('Cluster ID', fontsize=12)\n",
        "axes[0].set_ylabel('Number of Samples', fontsize=12)\n",
        "axes[0].set_title(f'Cluster Distribution\\n({best_feat_name} + {best_algo_name})', fontsize=13, fontweight='bold')\n",
        "axes[0].grid(True, alpha=0.3, axis='y') # Add horizontal grid lines for readability\n",
        "\n",
        "# Plot 2: Cluster vs. True Label Distribution (Conditional Heatmap)\n",
        "# This heatmap shows the overlap between assigned clusters and actual ground truth labels (if available).\n",
        "# This helps assess the semantic meaning of the discovered clusters.\n",
        "if true_labels is not None and len(true_labels) == len(final_clusters): # Check if true_labels exist and match length\n",
        "    print(\"\\nGenerating Cluster vs True Label Distribution Heatmap...\")\n",
        "    confusion_matrix = pd.crosstab(final_clusters, true_labels)\n",
        "    sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', ax=axes[1], cbar_kws={'label': 'Count'})\n",
        "    axes[1].set_xlabel('True Labels', fontsize=12)\n",
        "    axes[1].set_ylabel('Assigned Cluster ID', fontsize=12)\n",
        "    axes[1].set_title('Cluster vs True Label Distribution', fontsize=13, fontweight='bold')\n",
        "else:\n",
        "    print(\"\\nSkipping Cluster vs True Label Distribution Heatmap (True labels not available or mismatched length).\")\n",
        "    axes[1].text(0.5, 0.5, 'Ground Truth Labels\\nNot Available', # Display message in subplot\n",
        "                ha='center', va='center', fontsize=14, transform=axes[1].transAxes)\n",
        "    axes[1].axis('off') # Turn off axis for the empty plot\n",
        "\n",
        "# --- 3. Final Plot Adjustments and Saving ---\n",
        "plt.tight_layout() # Adjust layout to prevent overlapping elements.\n",
        "\n",
        "# Ensure the results directory exists before saving.\n",
        "visualization_dir = os.path.join(base_path, 'results/visualizations')\n",
        "os.makedirs(visualization_dir, exist_ok=True)\n",
        "\n",
        "# Save the generated figure to a file.\n",
        "plt.savefig(os.path.join(visualization_dir, 'cluster_distribution.png'), dpi=300, bbox_inches='tight')\n",
        "\n",
        "# Display the plot.\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QevjSvSLcqnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8474b460"
      },
      "source": [
        "# ========================================\n",
        "# CELL 23: Generate Report Summary\n",
        "# ========================================\n",
        "\n",
        "import os\n",
        "import pandas as pd # Ensure pandas is imported\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"GENERATING REPORT SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# --- 1. Ensure Necessary Variables for Summary Generation ---\n",
        "# The 'best_overall' DataFrame row should be available from CELL 17 analysis.\n",
        "# The 'results_df' DataFrame should be available from CELL 12 or CELL 13.\n",
        "# 'latent_dim' and 'n_clusters' should be available from earlier cells.\n",
        "\n",
        "# Re-evaluate best_overall and average metrics if for some reason previous cells were not run\n",
        "if 'best_overall' not in locals() or best_overall.empty:\n",
        "    results_df_numeric_sil = results_df.copy()\n",
        "    results_df_numeric_sil['Silhouette_Score'] = pd.to_numeric(results_df_numeric_sil['Silhouette_Score'], errors='coerce')\n",
        "    if not results_df_numeric_sil['Silhouette_Score'].dropna().empty:\n",
        "        best_overall = results_df_numeric_sil.loc[results_df_numeric_sil['Silhouette_Score'].idxmax()]\n",
        "    else:\n",
        "        best_overall = pd.Series({'Feature_Set': 'N/A', 'Algorithm': 'N/A', 'Silhouette_Score': 0, 'Davies_Bouldin_Index': 0, 'Calinski_Harabasz_Index': 0})\n",
        "\n",
        "# Recalculate average performance for feature sets if not already present\n",
        "if 'conv_vae_methods' not in locals() or 'hybrid_vae_methods' not in locals():\n",
        "    results_df_for_avg = results_df.copy()\n",
        "    for col in ['Silhouette_Score', 'Davies_Bouldin_Index', 'Calinski_Harabasz_Index', 'ARI']:\n",
        "        results_df_for_avg[col] = pd.to_numeric(results_df_for_avg[col], errors='coerce')\n",
        "    conv_vae_methods = results_df_for_avg[results_df_for_avg['Feature_Set'] == 'Conv-VAE']\n",
        "    hybrid_vae_methods = results_df_for_avg[results_df_for_avg['Feature_Set'] == 'Hybrid-VAE']\n",
        "\n",
        "# Determine if Hybrid-VAE or Conv-VAE performed better on average\n",
        "hybrid_avg_sil = hybrid_vae_methods['Silhouette_Score'].mean() if not hybrid_vae_methods['Silhouette_Score'].dropna().empty else 0\n",
        "conv_avg_sil = conv_vae_methods['Silhouette_Score'].mean() if not conv_vae_methods['Silhouette_Score'].dropna().empty else 0\n",
        "\n",
        "# --- 2. Construct the Report Summary String ---\n",
        "report_summary = f\"\"\"\n",
        "MEDIUM TASK - COMPREHENSIVE REPORT SUMMARY\n",
        "==========================================\n",
        "\n",
        "IMPLEMENTATION DETAILS:\n",
        "-----------------------\n",
        "- **Convolutional VAE:** Architecture designed for audio spectrograms, features 4 Conv2D layers.\n",
        "- **Hybrid VAE:** Combines latent representations from audio spectrograms (via pre-trained Conv-VAE encoder) and lyrics embeddings (via dense layers).\n",
        "- **Latent Dimension:** {latent_dim} for both VAEs.\n",
        "- **Training Epochs:** 50 for each VAE model.\n",
        "- **Batch Size:** 32 for each VAE model.\n",
        "\n",
        "CLUSTERING METHODS TESTED:\n",
        "--------------------------\n",
        "- **K-Means:** Partitioning algorithm (n_clusters={n_clusters}).\n",
        "- **Agglomerative Clustering:** Hierarchical algorithm (n_clusters={n_clusters}).\n",
        "- **DBSCAN:** Density-based spatial clustering (eps=0.5, min_samples=5).\n",
        "\n",
        "BEST PERFORMING CLUSTERING METHOD (by Silhouette Score):\n",
        "-------------------------------------------------------\n",
        "- **Feature Set:** {best_overall['Feature_Set']}\n",
        "- **Algorithm:** {best_overall['Algorithm']}\n",
        "- **Silhouette Score:** {best_overall['Silhouette_Score']:.4f}\n",
        "- **Davies-Bouldin Index:** {best_overall['Davies_Bouldin_Index']:.4f}\n",
        "- **Calinski-Harabasz Index:** {best_overall['Calinski_Harabasz_Index']:.4f}\n",
        "\n",
        "FEATURE SET COMPARISON (Average Silhouette Score across algorithms):\n",
        "-------------------------------------------------------------------\n",
        "- **Conv-VAE Features Average Silhouette:** {conv_avg_sil:.4f}\n",
        "- **Hybrid-VAE Features Average Silhouette:** {hybrid_avg_sil:.4f}\n",
        "\n",
        "KEY FINDINGS:\n",
        "---------------\n",
        "1.  {''}\n",
        "    { 'Hybrid-VAE features showed a slightly higher average Silhouette Score (%.4f) compared to Conv-VAE features (%.4f), indicating potential benefit from multimodal fusion.' % (hybrid_avg_sil, conv_avg_sil) if hybrid_avg_sil > conv_avg_sil else\n",
        "        'Conv-VAE features showed a slightly higher average Silhouette Score (%.4f) compared to Hybrid-VAE features (%.4f), suggesting audio-only features were more effective for clustering in this setup.' % (conv_avg_sil, hybrid_avg_sil) if conv_avg_sil > hybrid_avg_sil else\n",
        "        'Both Conv-VAE and Hybrid-VAE features performed comparably on average.'}\n",
        "2.  The **{best_overall['Algorithm']}** algorithm consistently performed well, identified as the best overall method with the highest Silhouette Score.\n",
        "3.  DBSCAN struggled with the default parameters, often finding only noise or a single cluster, indicating potential need for parameter tuning or unsuitability for the data's density distribution.\n",
        "4.  Adjusted Rand Index (ARI) could not be fully utilized as ground truth labels were not directly available or aligned for comparison, limiting external validation of clustering quality.\n",
        "\n",
        "FILES GENERATED & SAVED TO GOOGLE DRIVE ({base_path}results/):\n",
        "----------------------------------------------------------\n",
        " Conv-VAE model, encoder, and decoder (.h5 files)\n",
        " Hybrid-VAE model, encoder, and decoder (.h5 files)\n",
        " Extracted Conv-VAE latent features (latent_features_conv.npy)\n",
        " Extracted Hybrid-VAE latent features (latent_features_hybrid.npy)\n",
        " Comprehensive clustering metrics (comprehensive_clustering_metrics.csv)\n",
        " Training history plots for Conv-VAE and Hybrid-VAE (PNGs)\n",
        " t-SNE and UMAP visualizations for best methods (PNGs)\n",
        " Conv-VAE reconstruction quality visualization (PNG)\n",
        " Conv-VAE latent space interpolation visualization (PNG)\n",
        " Cluster distribution analysis plot (PNG)\n",
        " Analysis report (analysis_report.txt)\n",
        " This report summary (report_summary.txt)\n",
        "\n",
        "NEXT STEPS FOR RESEARCH PAPER:\n",
        "--------------------------------\n",
        "1.  **Introduction & Motivation:** Clearly articulate the problem and the rationale behind using VAEs for multimodal music feature extraction.\n",
        "2.  **Architectural Details:** Provide in-depth descriptions of the Conv-VAE and Hybrid VAE models, including layer configurations and hyperparameters.\n",
        "3.  **Experimental Setup & Results:** Present all quantitative metrics (Silhouette, Davies-Bouldin, Calinski-Harabasz) and visualizations (t-SNE, UMAP, reconstruction, interpolation, cluster distribution).\n",
        "4.  **Discussion:** Analyze why certain VAE architectures or clustering algorithms performed better/worse. Discuss the impact of multimodal fusion on latent representations and clustering quality.\n",
        "5.  **Limitations & Future Work:** Acknowledge limitations (e.g., lack of external validation metrics, sensitivity of DBSCAN) and suggest directions for future research (e.g., exploring optimal `n_clusters`, advanced multimodal fusion techniques, different evaluation metrics).\n",
        "6.  **Conclusion:** Summarize key findings and contributions.\n",
        "\"\"\"\n",
        "\n",
        "# Print the constructed report summary.\n",
        "print(report_summary)\n",
        "\n",
        "# --- 3. Save the Report Summary ---\n",
        "# Ensure the results directory exists.\n",
        "results_dir_path = os.path.join(base_path, 'results')\n",
        "os.makedirs(results_dir_path, exist_ok=True)\n",
        "\n",
        "# Define the full path for the report summary file.\n",
        "output_summary_path = os.path.join(results_dir_path, 'report_summary.txt')\n",
        "\n",
        "# Write the report summary to a text file.\n",
        "with open(output_summary_path, 'w') as f:\n",
        "    f.write(report_summary)\n",
        "\n",
        "print(f\"\\n Report summary saved to: {output_summary_path}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}